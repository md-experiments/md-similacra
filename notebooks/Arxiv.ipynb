{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "389a3ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "import os\n",
    "\n",
    "os.chdir(os.path.abspath(os.curdir).replace('notebooks',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e8f281a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import json\n",
    "from src.utils import files_in_dir\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Environment Variables\n",
    "import os\n",
    "\n",
    "with open('openai.key','r') as f:\n",
    "    openai_api_key = f.read()\n",
    "openai.api_key = openai_api_key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47d94b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f93cc188",
   "metadata": {},
   "outputs": [],
   "source": [
    "ARXIV_MAX_QUERY_LENGTH = 300\n",
    "\n",
    "query = 'retrieval augmented generation'\n",
    "top_k_results = 50\n",
    "docs = [\n",
    "    {\"published\": result.published.date(),\n",
    "     \"updated\": result.updated.date(),\n",
    "     \"entry_id\": result.entry_id,\n",
    "     \"Title\": result.title,\n",
    "    f\"Authors\": [a.name for a in result.authors],\n",
    "    f\"Summary\": result.summary}\n",
    "    for result in arxiv.Search(  # type: ignore\n",
    "        query[: ARXIV_MAX_QUERY_LENGTH], max_results=top_k_results\n",
    "    ).results()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef28394a",
   "metadata": {},
   "outputs": [],
   "source": [
    "http://arxiv.org/abs/2202.01110v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78dd0709",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'published': datetime.date(2022, 2, 2),\n",
       "  'updated': datetime.date(2022, 2, 13),\n",
       "  'entry_id': 'http://arxiv.org/abs/2202.01110v2',\n",
       "  'Title': 'A Survey on Retrieval-Augmented Text Generation',\n",
       "  'Authors': ['Huayang Li', 'Yixuan Su', 'Deng Cai', 'Yan Wang', 'Lemao Liu'],\n",
       "  'Summary': 'Recently, retrieval-augmented text generation attracted increasing attention\\nof the computational linguistics community. Compared with conventional\\ngeneration models, retrieval-augmented text generation has remarkable\\nadvantages and particularly has achieved state-of-the-art performance in many\\nNLP tasks. This paper aims to conduct a survey about retrieval-augmented text\\ngeneration. It firstly highlights the generic paradigm of retrieval-augmented\\ngeneration, and then it reviews notable approaches according to different tasks\\nincluding dialogue response generation, machine translation, and other\\ngeneration tasks. Finally, it points out some important directions on top of\\nrecent methods to facilitate future research.'},\n",
       " {'published': datetime.date(2023, 5, 11),\n",
       "  'updated': datetime.date(2023, 5, 11),\n",
       "  'entry_id': 'http://arxiv.org/abs/2305.06983v1',\n",
       "  'Title': 'Active Retrieval Augmented Generation',\n",
       "  'Authors': ['Zhengbao Jiang',\n",
       "   'Frank F. Xu',\n",
       "   'Luyu Gao',\n",
       "   'Zhiqing Sun',\n",
       "   'Qian Liu',\n",
       "   'Jane Dwivedi-Yu',\n",
       "   'Yiming Yang',\n",
       "   'Jamie Callan',\n",
       "   'Graham Neubig'],\n",
       "  'Summary': 'Despite the remarkable ability of large language models (LMs) to comprehend\\nand generate language, they have a tendency to hallucinate and create factually\\ninaccurate output. Augmenting LMs by retrieving information from external\\nknowledge resources is one promising solution. Most existing\\nretrieval-augmented LMs employ a retrieve-and-generate setup that only\\nretrieves information once based on the input. This is limiting, however, in\\nmore general scenarios involving generation of long texts, where continually\\ngathering information throughout the generation process is essential. There\\nhave been some past efforts to retrieve information multiple times while\\ngenerating outputs, which mostly retrieve documents at fixed intervals using\\nthe previous context as queries. In this work, we provide a generalized view of\\nactive retrieval augmented generation, methods that actively decide when and\\nwhat to retrieve across the course of the generation. We propose\\nForward-Looking Active REtrieval augmented generation (FLARE), a generic\\nretrieval-augmented generation method which iteratively uses a prediction of\\nthe upcoming sentence to anticipate future content, which is then utilized as a\\nquery to retrieve relevant documents to regenerate the sentence if it contains\\nlow-confidence tokens. We test FLARE along with baselines comprehensively over\\n4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves\\nsuperior or competitive performance on all tasks, demonstrating the\\neffectiveness of our method. Code and datasets are available at\\nhttps://github.com/jzbjyb/FLARE.'},\n",
       " {'published': datetime.date(2023, 5, 27),\n",
       "  'updated': datetime.date(2023, 5, 27),\n",
       "  'entry_id': 'http://arxiv.org/abs/2305.17331v1',\n",
       "  'Title': 'Augmentation-Adapted Retriever Improves Generalization of Language Models as Generic Plug-In',\n",
       "  'Authors': ['Zichun Yu', 'Chenyan Xiong', 'Shi Yu', 'Zhiyuan Liu'],\n",
       "  'Summary': \"Retrieval augmentation can aid language models (LMs) in knowledge-intensive\\ntasks by supplying them with external information. Prior works on retrieval\\naugmentation usually jointly fine-tune the retriever and the LM, making them\\nclosely coupled. In this paper, we explore the scheme of generic retrieval\\nplug-in: the retriever is to assist target LMs that may not be known beforehand\\nor are unable to be fine-tuned together. To retrieve useful documents for\\nunseen target LMs, we propose augmentation-adapted retriever (AAR), which\\nlearns LM's preferences obtained from a known source LM. Experiments on the\\nMMLU and PopQA datasets demonstrate that our AAR trained with a small source LM\\nis able to significantly improve the zero-shot generalization of larger target\\nLMs ranging from 250M Flan-T5 to 175B InstructGPT. Further analysis indicates\\nthat the preferences of different LMs overlap, enabling AAR trained with a\\nsingle source LM to serve as a generic plug-in for various target LMs. Our code\\nis open-sourced at https://github.com/OpenMatch/Augmentation-Adapted-Retriever.\"},\n",
       " {'published': datetime.date(2023, 2, 7),\n",
       "  'updated': datetime.date(2023, 2, 7),\n",
       "  'entry_id': 'http://arxiv.org/abs/2302.03754v1',\n",
       "  'Title': 'Augmenting Zero-Shot Dense Retrievers with Plug-in Mixture-of-Memories',\n",
       "  'Authors': ['Suyu Ge',\n",
       "   'Chenyan Xiong',\n",
       "   'Corby Rosset',\n",
       "   'Arnold Overwijk',\n",
       "   'Jiawei Han',\n",
       "   'Paul Bennett'],\n",
       "  'Summary': 'In this paper we improve the zero-shot generalization ability of language\\nmodels via Mixture-Of-Memory Augmentation (MoMA), a mechanism that retrieves\\naugmentation documents from multiple information corpora (\"external memories\"),\\nwith the option to \"plug in\" new memory at inference time. We develop a joint\\nlearning mechanism that trains the augmentation component with latent labels\\nderived from the end retrieval task, paired with hard negatives from the memory\\nmixture. We instantiate the model in a zero-shot dense retrieval setting by\\naugmenting a strong T5-based retriever with MoMA. Our model, MoMA, obtains\\nstrong zero-shot retrieval accuracy on the eighteen tasks included in the\\nstandard BEIR benchmark. It outperforms systems that seek generalization from\\nincreased model parameters and computation steps. Our analysis further\\nillustrates the necessity of augmenting with mixture-of-memory for robust\\ngeneralization, the benefits of augmentation learning, and how MoMA utilizes\\nthe plug-in memory at inference time without changing its parameters. We plan\\nto open source our code.'},\n",
       " {'published': datetime.date(2022, 12, 18),\n",
       "  'updated': datetime.date(2023, 5, 7),\n",
       "  'entry_id': 'http://arxiv.org/abs/2212.09146v2',\n",
       "  'Title': 'Can Retriever-Augmented Language Models Reason? The Blame Game Between the Retriever and the Language Model',\n",
       "  'Authors': ['Parishad BehnamGhader', 'Santiago Miret', 'Siva Reddy'],\n",
       "  'Summary': \"Augmenting pretrained language models with retrievers to select the\\nsupporting documents has shown promise in effectively solving common NLP\\nproblems, including language modeling and question answering, in an\\ninterpretable way. In this paper, we first study the strengths and weaknesses\\nof different retriever-augmented language models (REALM, $k$NN-LM, FiD coupled\\nwith DPR, and ATLAS and Flan-T5 coupled with Contriever) in reasoning over the\\nretrieved statements in different tasks. We show how the retrieve-then-read\\nmodels' limitations in reasoning are rooted both in the retriever module as\\nwell as the language model. Our experimental results demonstrate that the\\nsimilarity metric used by the retrievers is generally insufficient for\\nreasoning tasks. Additionally, we show that the language models in\\nretriever-augmented models do not take the complicated relations between the\\nstatements into account, which leads to poor reasoning performance even when\\nusing the larger models. Moreover, we analyze the reasoning performance of\\nlarge language models using multihop retrieval but we only observe minor\\nimprovements. Overall, this shows great room for further research in this area.\"},\n",
       " {'published': datetime.date(2023, 5, 24),\n",
       "  'updated': datetime.date(2023, 5, 24),\n",
       "  'entry_id': 'http://arxiv.org/abs/2305.15294v1',\n",
       "  'Title': 'Enhancing Retrieval-Augmented Large Language Models with Iterative Retrieval-Generation Synergy',\n",
       "  'Authors': ['Zhihong Shao',\n",
       "   'Yeyun Gong',\n",
       "   'Yelong Shen',\n",
       "   'Minlie Huang',\n",
       "   'Nan Duan',\n",
       "   'Weizhu Chen'],\n",
       "  'Summary': 'Large language models are powerful text processors and reasoners, but are\\nstill subject to limitations including outdated knowledge and hallucinations,\\nwhich necessitates connecting them to the world. Retrieval-augmented large\\nlanguage models have raised extensive attention for grounding model generation\\non external knowledge. However, retrievers struggle to capture relevance,\\nespecially for queries with complex information needs. Recent work has proposed\\nto improve relevance modeling by having large language models actively involved\\nin retrieval, i.e., to improve retrieval with generation. In this paper, we\\nshow that strong performance can be achieved by a method we call Iter-RetGen,\\nwhich synergizes retrieval and generation in an iterative manner. A model\\noutput shows what might be needed to finish a task, and thus provides an\\ninformative context for retrieving more relevant knowledge which in turn helps\\ngenerate a better output in the next iteration. Compared with recent work which\\ninterleaves retrieval with generation when producing an output, Iter-RetGen\\nprocesses all retrieved knowledge as a whole and largely preserves the\\nflexibility in generation without structural constraints. We evaluate\\nIter-RetGen on multi-hop question answering, fact verification, and commonsense\\nreasoning, and show that it can flexibly leverage parametric knowledge and\\nnon-parametric knowledge, and is superior to or competitive with\\nstate-of-the-art retrieval-augmented baselines while causing fewer overheads of\\nretrieval and generation. We can further improve performance via\\ngeneration-augmented retrieval adaptation.'},\n",
       " {'published': datetime.date(2022, 7, 29),\n",
       "  'updated': datetime.date(2022, 7, 29),\n",
       "  'entry_id': 'http://arxiv.org/abs/2207.14428v1',\n",
       "  'Title': 'Paired Cross-Modal Data Augmentation for Fine-Grained Image-to-Text Retrieval',\n",
       "  'Authors': ['Hao Wang', 'Guosheng Lin', 'Steven C. H. Hoi', 'Chunyan Miao'],\n",
       "  'Summary': 'This paper investigates an open research problem of generating text-image\\npairs to improve the training of fine-grained image-to-text cross-modal\\nretrieval task, and proposes a novel framework for paired data augmentation by\\nuncovering the hidden semantic information of StyleGAN2 model. Specifically, we\\nfirst train a StyleGAN2 model on the given dataset. We then project the real\\nimages back to the latent space of StyleGAN2 to obtain the latent codes. To\\nmake the generated images manipulatable, we further introduce a latent space\\nalignment module to learn the alignment between StyleGAN2 latent codes and the\\ncorresponding textual caption features. When we do online paired data\\naugmentation, we first generate augmented text through random token\\nreplacement, then pass the augmented text into the latent space alignment\\nmodule to output the latent codes, which are finally fed to StyleGAN2 to\\ngenerate the augmented images. We evaluate the efficacy of our augmented data\\napproach on two public cross-modal retrieval datasets, in which the promising\\nexperimental results demonstrate the augmented text-image pair data can be\\ntrained together with the original data to boost the image-to-text cross-modal\\nretrieval performance.'},\n",
       " {'published': datetime.date(2023, 2, 22),\n",
       "  'updated': datetime.date(2023, 2, 22),\n",
       "  'entry_id': 'http://arxiv.org/abs/2302.11352v1',\n",
       "  'Title': 'X-TRA: Improving Chest X-ray Tasks with Cross-Modal Retrieval Augmentation',\n",
       "  'Authors': ['Tom van Sonsbeek', 'Marcel Worring'],\n",
       "  'Summary': 'An important component of human analysis of medical images and their context\\nis the ability to relate newly seen things to related instances in our memory.\\nIn this paper we mimic this ability by using multi-modal retrieval augmentation\\nand apply it to several tasks in chest X-ray analysis. By retrieving similar\\nimages and/or radiology reports we expand and regularize the case at hand with\\nadditional knowledge, while maintaining factual knowledge consistency. The\\nmethod consists of two components. First, vision and language modalities are\\naligned using a pre-trained CLIP model. To enforce that the retrieval focus\\nwill be on detailed disease-related content instead of global visual appearance\\nit is fine-tuned using disease class information. Subsequently, we construct a\\nnon-parametric retrieval index, which reaches state-of-the-art retrieval\\nlevels. We use this index in our downstream tasks to augment image\\nrepresentations through multi-head attention for disease classification and\\nreport retrieval. We show that retrieval augmentation gives considerable\\nimprovements on these tasks. Our downstream report retrieval even shows to be\\ncompetitive with dedicated report generation methods, paving the path for this\\nmethod in medical imaging.'},\n",
       " {'published': datetime.date(2023, 6, 8),\n",
       "  'updated': datetime.date(2023, 6, 8),\n",
       "  'entry_id': 'http://arxiv.org/abs/2306.05212v1',\n",
       "  'Title': 'RETA-LLM: A Retrieval-Augmented Large Language Model Toolkit',\n",
       "  'Authors': ['Jiongnan Liu',\n",
       "   'Jiajie Jin',\n",
       "   'Zihan Wang',\n",
       "   'Jiehan Cheng',\n",
       "   'Zhicheng Dou',\n",
       "   'Ji-Rong Wen'],\n",
       "  'Summary': 'Although Large Language Models (LLMs) have demonstrated extraordinary\\ncapabilities in many domains, they still have a tendency to hallucinate and\\ngenerate fictitious responses to user requests. This problem can be alleviated\\nby augmenting LLMs with information retrieval (IR) systems (also known as\\nretrieval-augmented LLMs). Applying this strategy, LLMs can generate more\\nfactual texts in response to user input according to the relevant content\\nretrieved by IR systems from external corpora as references. In addition, by\\nincorporating external knowledge, retrieval-augmented LLMs can answer in-domain\\nquestions that cannot be answered by solely relying on the world knowledge\\nstored in parameters. To support research in this area and facilitate the\\ndevelopment of retrieval-augmented LLM systems, we develop RETA-LLM, a\\n{RET}reival-{A}ugmented LLM toolkit. In RETA-LLM, we create a complete pipeline\\nto help researchers and users build their customized in-domain LLM-based\\nsystems. Compared with previous retrieval-augmented LLM systems, RETA-LLM\\nprovides more plug-and-play modules to support better interaction between IR\\nsystems and LLMs, including {request rewriting, document retrieval, passage\\nextraction, answer generation, and fact checking} modules. Our toolkit is\\npublicly available at https://github.com/RUC-GSAI/YuLan-IR/tree/main/RETA-LLM.'},\n",
       " {'published': datetime.date(2022, 5, 21),\n",
       "  'updated': datetime.date(2022, 6, 1),\n",
       "  'entry_id': 'http://arxiv.org/abs/2205.10471v2',\n",
       "  'Title': 'Retrieval-Augmented Multilingual Keyphrase Generation with Retriever-Generator Iterative Training',\n",
       "  'Authors': ['Yifan Gao',\n",
       "   'Qingyu Yin',\n",
       "   'Zheng Li',\n",
       "   'Rui Meng',\n",
       "   'Tong Zhao',\n",
       "   'Bing Yin',\n",
       "   'Irwin King',\n",
       "   'Michael R. Lyu'],\n",
       "  'Summary': \"Keyphrase generation is the task of automatically predicting keyphrases given\\na piece of long text. Despite its recent flourishing, keyphrase generation on\\nnon-English languages haven't been vastly investigated. In this paper, we call\\nattention to a new setting named multilingual keyphrase generation and we\\ncontribute two new datasets, EcommerceMKP and AcademicMKP, covering six\\nlanguages. Technically, we propose a retrieval-augmented method for\\nmultilingual keyphrase generation to mitigate the data shortage problem in\\nnon-English languages. The retrieval-augmented model leverages keyphrase\\nannotations in English datasets to facilitate generating keyphrases in\\nlow-resource languages. Given a non-English passage, a cross-lingual dense\\npassage retrieval module finds relevant English passages. Then the associated\\nEnglish keyphrases serve as external knowledge for keyphrase generation in the\\ncurrent language. Moreover, we develop a retriever-generator iterative training\\nalgorithm to mine pseudo parallel passage pairs to strengthen the cross-lingual\\npassage retriever. Comprehensive experiments and ablations show that the\\nproposed approach outperforms all baselines.\"},\n",
       " {'published': datetime.date(2021, 12, 10),\n",
       "  'updated': datetime.date(2021, 12, 10),\n",
       "  'entry_id': 'http://arxiv.org/abs/2112.07618v1',\n",
       "  'Title': 'Robust Information Retrieval for False Claims with Distracting Entities In Fact Extraction and Verification',\n",
       "  'Authors': ['Mingwen Dong',\n",
       "   'Christos Christodoulopoulos',\n",
       "   'Sheng-Min Shih',\n",
       "   'Xiaofei Ma'],\n",
       "  'Summary': \"Accurate evidence retrieval is essential for automated fact checking. Little\\nprevious research has focused on the differences between true and false claims\\nand how they affect evidence retrieval. This paper shows that, compared with\\ntrue claims, false claims more frequently contain irrelevant entities which can\\ndistract evidence retrieval model. A BERT-based retrieval model made more\\nmistakes in retrieving refuting evidence for false claims than supporting\\nevidence for true claims. When tested with adversarial false claims\\n(synthetically generated) containing irrelevant entities, the recall of the\\nretrieval model is significantly lower than that for original claims. These\\nresults suggest that the vanilla BERT-based retrieval model is not robust to\\nirrelevant entities in the false claims. By augmenting the training data with\\nsynthetic false claims containing irrelevant entities, the trained model\\nachieved higher evidence recall, including that of false claims with irrelevant\\nentities. In addition, using separate models to retrieve refuting and\\nsupporting evidence and then aggregating them can also increase the evidence\\nrecall, including that of false claims with irrelevant entities. These results\\nsuggest that we can increase the BERT-based retrieval model's robustness to\\nfalse claims with irrelevant entities via data augmentation and model ensemble.\"},\n",
       " {'published': datetime.date(2023, 6, 23),\n",
       "  'updated': datetime.date(2023, 6, 23),\n",
       "  'entry_id': 'http://arxiv.org/abs/2306.13421v1',\n",
       "  'Title': 'Long-range Language Modeling with Self-retrieval',\n",
       "  'Authors': ['Ohad Rubin', 'Jonathan Berant'],\n",
       "  'Summary': 'Retrieval-augmented language models (LMs) have received much attention\\nrecently. However, typically the retriever is not trained jointly as a native\\ncomponent of the LM, but added to an already-pretrained LM, which limits the\\nability of the LM and the retriever to adapt to one another. In this work, we\\npropose the Retrieval-Pretrained Transformer (RPT), an architecture and\\ntraining procedure for jointly training a retrieval-augmented LM from scratch\\nfor the task of modeling long texts. Given a recently generated text chunk in a\\nlong document, the LM computes query representations, which are then used to\\nretrieve earlier chunks in the document, located potentially tens of thousands\\nof tokens before. Information from retrieved chunks is fused into the LM\\nrepresentations to predict the next target chunk. We train the retriever\\ncomponent with a semantic objective, where the goal is to retrieve chunks that\\nincrease the probability of the next chunk, according to a reference LM. We\\nevaluate RPT on four long-range language modeling tasks, spanning books, code,\\nand mathematical writing, and demonstrate that RPT improves retrieval quality\\nand subsequently perplexity across the board compared to strong baselines.'},\n",
       " {'published': datetime.date(2023, 5, 28),\n",
       "  'updated': datetime.date(2023, 5, 28),\n",
       "  'entry_id': 'http://arxiv.org/abs/2305.17653v1',\n",
       "  'Title': 'Prompt-Guided Retrieval Augmentation for Non-Knowledge-Intensive Tasks',\n",
       "  'Authors': ['Zhicheng Guo',\n",
       "   'Sijie Cheng',\n",
       "   'Yile Wang',\n",
       "   'Peng Li',\n",
       "   'Yang Liu'],\n",
       "  'Summary': 'Retrieval-augmented methods have received increasing attention to support\\ndownstream tasks by leveraging useful information from external resources.\\nRecent studies mainly focus on exploring retrieval to solve knowledge-intensive\\n(KI) tasks. However, the potential of retrieval for most\\nnon-knowledge-intensive (NKI) tasks remains under-explored. There are two main\\nchallenges to leveraging retrieval-augmented methods for NKI tasks: 1) the\\ndemand for diverse relevance score functions and 2) the dilemma between\\ntraining cost and task performance. To address these challenges, we propose a\\ntwo-stage framework for NKI tasks, named PGRA. In the first stage, we adopt a\\ntask-agnostic retriever to build a shared static index and select candidate\\nevidence efficiently. In the second stage, we design a prompt-guided reranker\\nto rerank the nearest evidence according to task-specific relevance for the\\nreader. Experimental results show that PGRA outperforms other state-of-the-art\\nretrieval-augmented methods. Our analyses further investigate the influence\\nfactors to model performance and demonstrate the generality of PGRA. Codes are\\navailable at https://github.com/THUNLP-MT/PGRA.'},\n",
       " {'published': datetime.date(2022, 10, 23),\n",
       "  'updated': datetime.date(2022, 10, 23),\n",
       "  'entry_id': 'http://arxiv.org/abs/2210.12887v1',\n",
       "  'Title': 'Retrieval Augmentation for Commonsense Reasoning: A Unified Approach',\n",
       "  'Authors': ['Wenhao Yu',\n",
       "   'Chenguang Zhu',\n",
       "   'Zhihan Zhang',\n",
       "   'Shuohang Wang',\n",
       "   'Zhuosheng Zhang',\n",
       "   'Yuwei Fang',\n",
       "   'Meng Jiang'],\n",
       "  'Summary': 'A common thread of retrieval-augmented methods in the existing literature\\nfocuses on retrieving encyclopedic knowledge, such as Wikipedia, which\\nfacilitates well-defined entity and relation spaces that can be modeled.\\nHowever, applying such methods to commonsense reasoning tasks faces two unique\\nchallenges, i.e., the lack of a general large-scale corpus for retrieval and a\\ncorresponding effective commonsense retriever. In this paper, we systematically\\ninvestigate how to leverage commonsense knowledge retrieval to improve\\ncommonsense reasoning tasks. We proposed a unified framework of\\nretrieval-augmented commonsense reasoning (called RACo), including a newly\\nconstructed commonsense corpus with over 20 million documents and novel\\nstrategies for training a commonsense retriever. We conducted experiments on\\nfour different commonsense reasoning tasks. Extensive evaluation results showed\\nthat our proposed RACo can significantly outperform other knowledge-enhanced\\nmethod counterparts, achieving new SoTA performance on the CommonGen and CREAK\\nleaderboards.'},\n",
       " {'published': datetime.date(2021, 8, 26),\n",
       "  'updated': datetime.date(2021, 9, 10),\n",
       "  'entry_id': 'http://arxiv.org/abs/2108.11601v2',\n",
       "  'Title': 'Retrieval Augmented Code Generation and Summarization',\n",
       "  'Authors': ['Md Rizwan Parvez',\n",
       "   'Wasi Uddin Ahmad',\n",
       "   'Saikat Chakraborty',\n",
       "   'Baishakhi Ray',\n",
       "   'Kai-Wei Chang'],\n",
       "  'Summary': \"Software developers write a lot of source code and documentation during\\nsoftware development. Intrinsically, developers often recall parts of source\\ncode or code summaries that they had written in the past while implementing\\nsoftware or documenting them. To mimic developers' code or summary generation\\nbehavior, we propose a retrieval augmented framework, REDCODER, that retrieves\\nrelevant code or summaries from a retrieval database and provides them as a\\nsupplement to code generation or summarization models. REDCODER has a couple of\\nuniqueness. First, it extends the state-of-the-art dense retrieval technique to\\nsearch for relevant code or summaries. Second, it can work with retrieval\\ndatabases that include unimodal (only code or natural language description) or\\nbimodal instances (code-description pairs). We conduct experiments and\\nextensive analysis on two benchmark datasets of code generation and\\nsummarization in Java and Python, and the promising results endorse the\\neffectiveness of our proposed retrieval augmented framework.\"},\n",
       " {'published': datetime.date(2023, 7, 7),\n",
       "  'updated': datetime.date(2023, 7, 7),\n",
       "  'entry_id': 'http://arxiv.org/abs/2307.04642v1',\n",
       "  'Title': 'TRAC: Trustworthy Retrieval Augmented Chatbot',\n",
       "  'Authors': ['Shuo Li', 'Sangdon Park', 'Insup Lee', 'Osbert Bastani'],\n",
       "  'Summary': 'Although conversational AIs have demonstrated fantastic performance, they\\noften generate incorrect information, or hallucinations. Retrieval augmented\\ngeneration has emerged as a promising solution to reduce these hallucinations.\\nHowever, these techniques still cannot guarantee correctness. Focusing on\\nquestion answering, we propose a framework that can provide statistical\\nguarantees for the retrieval augmented question answering system by combining\\nconformal prediction and global testing. In addition, we use Bayesian\\noptimization to choose hyperparameters of the global test to maximize the\\nperformance of the system. Our empirical results on the Natural Questions\\ndataset demonstrate that our method can provide the desired coverage guarantee\\nwhile minimizing the average prediction set size.'},\n",
       " {'published': datetime.date(2021, 4, 15),\n",
       "  'updated': datetime.date(2022, 1, 21),\n",
       "  'entry_id': 'http://arxiv.org/abs/2104.07713v2',\n",
       "  'Title': 'Contrastive Learning with Stronger Augmentations',\n",
       "  'Authors': ['Xiao Wang', 'Guo-Jun Qi'],\n",
       "  'Summary': \"Representation learning has significantly been developed with the advance of\\ncontrastive learning methods. Most of those methods have benefited from various\\ndata augmentations that are carefully designated to maintain their identities\\nso that the images transformed from the same instance can still be retrieved.\\nHowever, those carefully designed transformations limited us to further explore\\nthe novel patterns exposed by other transformations. Meanwhile, as found in our\\nexperiments, the strong augmentations distorted the images' structures,\\nresulting in difficult retrieval. Thus, we propose a general framework called\\nContrastive Learning with Stronger Augmentations~(CLSA) to complement current\\ncontrastive learning approaches. Here, the distribution divergence between the\\nweakly and strongly augmented images over the representation bank is adopted to\\nsupervise the retrieval of strongly augmented queries from a pool of instances.\\nExperiments on the ImageNet dataset and downstream datasets showed the\\ninformation from the strongly augmented images can significantly boost the\\nperformance. For example, CLSA achieves top-1 accuracy of 76.2% on ImageNet\\nwith a standard ResNet-50 architecture with a single-layer classifier\\nfine-tuned, which is almost the same level as 76.5% of supervised results. The\\ncode and pre-trained models are available in\\nhttps://github.com/maple-research-lab/CLSA.\"},\n",
       " {'published': datetime.date(2022, 12, 20),\n",
       "  'updated': datetime.date(2022, 12, 20),\n",
       "  'entry_id': 'http://arxiv.org/abs/2212.10692v1',\n",
       "  'Title': 'Generation-Augmented Query Expansion For Code Retrieval',\n",
       "  'Authors': ['Dong Li',\n",
       "   'Yelong Shen',\n",
       "   'Ruoming Jin',\n",
       "   'Yi Mao',\n",
       "   'Kuan Wang',\n",
       "   'Weizhu Chen'],\n",
       "  'Summary': 'Pre-trained language models have achieved promising success in code retrieval\\ntasks, where a natural language documentation query is given to find the most\\nrelevant existing code snippet. However, existing models focus only on\\noptimizing the documentation code pairs by embedding them into latent space,\\nwithout the association of external knowledge. In this paper, we propose a\\ngeneration-augmented query expansion framework. Inspired by the human retrieval\\nprocess - sketching an answer before searching, in this work, we utilize the\\npowerful code generation model to benefit the code retrieval task.\\nSpecifically, we demonstrate that rather than merely retrieving the target code\\nsnippet according to the documentation query, it would be helpful to augment\\nthe documentation query with its generation counterpart - generated code\\nsnippets from the code generation model. To the best of our knowledge, this is\\nthe first attempt that leverages the code generation model to enhance the code\\nretrieval task. We achieve new state-of-the-art results on the CodeSearchNet\\nbenchmark and surpass the baselines significantly.'},\n",
       " {'published': datetime.date(2023, 5, 18),\n",
       "  'updated': datetime.date(2023, 5, 18),\n",
       "  'entry_id': 'http://arxiv.org/abs/2305.11074v1',\n",
       "  'Title': 'Tram: A Token-level Retrieval-augmented Mechanism for Source Code Summarization',\n",
       "  'Authors': ['Tong Ye',\n",
       "   'Lingfei Wu',\n",
       "   'Tengfei Ma',\n",
       "   'Xuhong Zhang',\n",
       "   'Yangkai Du',\n",
       "   'Peiyu Liu',\n",
       "   'Wenhai Wang',\n",
       "   'Shouling Ji'],\n",
       "  'Summary': 'Automatically generating human-readable text describing the functionality of\\na program is the intent of source code summarization. Although Neural Language\\nModels achieve significant performance in this field, an emerging trend is\\ncombining neural models with external knowledge. Most previous approaches rely\\non the sentence-level retrieval and combination paradigm (retrieval of similar\\ncode snippets and use of the corresponding code and summary pairs) on the\\nencoder side. However, this paradigm is coarse-grained and cannot directly take\\nadvantage of the high-quality retrieved summary tokens on the decoder side. In\\nthis paper, we explore a fine-grained token-level retrieval-augmented mechanism\\non the decoder side to help the vanilla neural model generate a better code\\nsummary. Furthermore, to mitigate the limitation of token-level retrieval on\\ncapturing contextual code semantics, we propose to integrate code semantics\\ninto summary tokens. Extensive experiments and human evaluation reveal that our\\ntoken-level retrieval-augmented approach significantly improves performance and\\nis more interpretive.'},\n",
       " {'published': datetime.date(2023, 3, 20),\n",
       "  'updated': datetime.date(2023, 3, 20),\n",
       "  'entry_id': 'http://arxiv.org/abs/2303.10868v1',\n",
       "  'Title': 'Retrieving Multimodal Information for Augmented Generation: A Survey',\n",
       "  'Authors': ['Ruochen Zhao',\n",
       "   'Hailin Chen',\n",
       "   'Weishi Wang',\n",
       "   'Fangkai Jiao',\n",
       "   'Xuan Long Do',\n",
       "   'Chengwei Qin',\n",
       "   'Bosheng Ding',\n",
       "   'Xiaobao Guo',\n",
       "   'Minzhi Li',\n",
       "   'Xingxuan Li',\n",
       "   'Shafiq Joty'],\n",
       "  'Summary': 'In this survey, we review methods that retrieve multimodal knowledge to\\nassist and augment generative models. This group of works focuses on retrieving\\ngrounding contexts from external sources, including images, codes, tables,\\ngraphs, and audio. As multimodal learning and generative AI have become more\\nand more impactful, such retrieval augmentation offers a promising solution to\\nimportant concerns such as factuality, reasoning, interpretability, and\\nrobustness. We provide an in-depth review of retrieval-augmented generation in\\ndifferent modalities and discuss potential future directions. As this is an\\nemerging field, we continue to add new papers and methods.'},\n",
       " {'published': datetime.date(2020, 9, 17),\n",
       "  'updated': datetime.date(2021, 8, 6),\n",
       "  'entry_id': 'http://arxiv.org/abs/2009.08553v4',\n",
       "  'Title': 'Generation-Augmented Retrieval for Open-domain Question Answering',\n",
       "  'Authors': ['Yuning Mao',\n",
       "   'Pengcheng He',\n",
       "   'Xiaodong Liu',\n",
       "   'Yelong Shen',\n",
       "   'Jianfeng Gao',\n",
       "   'Jiawei Han',\n",
       "   'Weizhu Chen'],\n",
       "  'Summary': 'We propose Generation-Augmented Retrieval (GAR) for answering open-domain\\nquestions, which augments a query through text generation of heuristically\\ndiscovered relevant contexts without external resources as supervision. We\\ndemonstrate that the generated contexts substantially enrich the semantics of\\nthe queries and GAR with sparse representations (BM25) achieves comparable or\\nbetter performance than state-of-the-art dense retrieval methods such as DPR.\\nWe show that generating diverse contexts for a query is beneficial as fusing\\ntheir results consistently yields better retrieval accuracy. Moreover, as\\nsparse and dense representations are often complementary, GAR can be easily\\ncombined with DPR to achieve even better performance. GAR achieves\\nstate-of-the-art performance on Natural Questions and TriviaQA datasets under\\nthe extractive QA setup when equipped with an extractive reader, and\\nconsistently outperforms other retrieval methods when the same generative\\nreader is used.'},\n",
       " {'published': datetime.date(2023, 5, 5),\n",
       "  'updated': datetime.date(2023, 5, 5),\n",
       "  'entry_id': 'http://arxiv.org/abs/2305.03660v1',\n",
       "  'Title': 'Retrieval Augmented Chest X-Ray Report Generation using OpenAI GPT models',\n",
       "  'Authors': ['Mercy Ranjit',\n",
       "   'Gopinath Ganapathy',\n",
       "   'Ranjit Manuel',\n",
       "   'Tanuja Ganu'],\n",
       "  'Summary': 'We propose Retrieval Augmented Generation (RAG) as an approach for automated\\nradiology report writing that leverages multimodally aligned embeddings from a\\ncontrastively pretrained vision language model for retrieval of relevant\\ncandidate radiology text for an input radiology image and a general domain\\ngenerative model like OpenAI text-davinci-003, gpt-3.5-turbo and gpt-4 for\\nreport generation using the relevant radiology text retrieved. This approach\\nkeeps hallucinated generations under check and provides capabilities to\\ngenerate report content in the format we desire leveraging the instruction\\nfollowing capabilities of these generative models. Our approach achieves better\\nclinical metrics with a BERTScore of 0.2865 ({\\\\Delta}+ 25.88%) and Semb score\\nof 0.4026 ({\\\\Delta}+ 6.31%). Our approach can be broadly relevant for different\\nclinical settings as it allows to augment the automated radiology report\\ngeneration process with content relevant for that setting while also having the\\nability to inject user intents and requirements in the prompts as part of the\\nreport generation process to modulate the content and format of the generated\\nreports as applicable for that clinical setting.'},\n",
       " {'published': datetime.date(2022, 9, 28),\n",
       "  'updated': datetime.date(2022, 9, 28),\n",
       "  'entry_id': 'http://arxiv.org/abs/2209.14290v1',\n",
       "  'Title': 'FiD-Light: Efficient and Effective Retrieval-Augmented Text Generation',\n",
       "  'Authors': ['Sebastian Hofstätter',\n",
       "   'Jiecao Chen',\n",
       "   'Karthik Raman',\n",
       "   'Hamed Zamani'],\n",
       "  'Summary': 'Retrieval-augmented generation models offer many benefits over standalone\\nlanguage models: besides a textual answer to a given query they provide\\nprovenance items retrieved from an updateable knowledge base. However, they are\\nalso more complex systems and need to handle long inputs. In this work, we\\nintroduce FiD-Light to strongly increase the efficiency of the state-of-the-art\\nretrieval-augmented FiD model, while maintaining the same level of\\neffectiveness. Our FiD-Light model constrains the information flow from the\\nencoder (which encodes passages separately) to the decoder (using concatenated\\nencoded representations). Furthermore, we adapt FiD-Light with re-ranking\\ncapabilities through textual source pointers, to improve the top-ranked\\nprovenance precision. Our experiments on a diverse set of seven knowledge\\nintensive tasks (KILT) show FiD-Light consistently improves the Pareto frontier\\nbetween query latency and effectiveness. FiD-Light with source pointing sets\\nsubstantial new state-of-the-art results on six KILT tasks for combined text\\ngeneration and provenance retrieval evaluation, while maintaining reasonable\\nefficiency.'},\n",
       " {'published': datetime.date(2023, 4, 13),\n",
       "  'updated': datetime.date(2023, 4, 13),\n",
       "  'entry_id': 'http://arxiv.org/abs/2304.06762v1',\n",
       "  'Title': 'Shall We Pretrain Autoregressive Language Models with Retrieval? A Comprehensive Study',\n",
       "  'Authors': ['Boxin Wang',\n",
       "   'Wei Ping',\n",
       "   'Peng Xu',\n",
       "   'Lawrence McAfee',\n",
       "   'Zihan Liu',\n",
       "   'Mohammad Shoeybi',\n",
       "   'Yi Dong',\n",
       "   'Oleksii Kuchaiev',\n",
       "   'Bo Li',\n",
       "   'Chaowei Xiao',\n",
       "   'Anima Anandkumar',\n",
       "   'Bryan Catanzaro'],\n",
       "  'Summary': 'Large decoder-only language models (LMs) can be largely improved in terms of\\nperplexity by retrieval (e.g., RETRO), but its impact on text generation\\nquality and downstream task accuracy is unclear. Thus, it is still an open\\nquestion: shall we pretrain large autoregressive LMs with retrieval? To answer\\nit, we perform a comprehensive study on a scalable pre-trained\\nretrieval-augmented LM (i.e., RETRO) compared with standard GPT and\\nretrieval-augmented GPT incorporated at fine-tuning or inference stages. We\\nfirst provide the recipe to reproduce RETRO up to 9.5B parameters while\\nretrieving a text corpus with 330B tokens. Based on that, we have the following\\nnovel findings: i) RETRO outperforms GPT on text generation with much less\\ndegeneration (i.e., repetition), moderately higher factual accuracy, and\\nslightly lower toxicity with a nontoxic retrieval database. ii) On the LM\\nEvaluation Harness benchmark, RETRO largely outperforms GPT on\\nknowledge-intensive tasks, but is on par with GPT on other tasks. Furthermore,\\nwe introduce a simple variant of the model, RETRO++, which largely improves\\nopen-domain QA results of original RETRO (e.g., EM score +8.6 on Natural\\nQuestion) and significantly outperforms retrieval-augmented GPT across\\ndifferent model sizes. Our findings highlight the promising direction of\\npretraining autoregressive LMs with retrieval as future foundation models. We\\nrelease our implementation at: https://github.com/NVIDIA/Megatron-LM#retro'},\n",
       " {'published': datetime.date(2023, 5, 3),\n",
       "  'updated': datetime.date(2023, 5, 17),\n",
       "  'entry_id': 'http://arxiv.org/abs/2305.02437v2',\n",
       "  'Title': 'Lift Yourself Up: Retrieval-augmented Text Generation with Self Memory',\n",
       "  'Authors': ['Xin Cheng',\n",
       "   'Di Luo',\n",
       "   'Xiuying Chen',\n",
       "   'Lemao Liu',\n",
       "   'Dongyan Zhao',\n",
       "   'Rui Yan'],\n",
       "  'Summary': 'With direct access to human-written reference as memory, retrieval-augmented\\ngeneration has achieved much progress in a wide range of text generation tasks.\\nSince better memory would typically prompt better generation~(we define this as\\nprimal problem). The traditional approach for memory retrieval involves\\nselecting memory that exhibits the highest similarity to the input. However,\\nthis method is constrained by the quality of the fixed corpus from which memory\\nis retrieved. In this paper, by exploring the duality of the primal problem:\\nbetter generation also prompts better memory, we propose a novel framework,\\nselfmem, which addresses this limitation by iteratively employing a\\nretrieval-augmented generator to create an unbounded memory pool and using a\\nmemory selector to choose one output as memory for the subsequent generation\\nround. This enables the model to leverage its own output, referred to as\\nself-memory, for improved generation. We evaluate the effectiveness of selfmem\\non three distinct text generation tasks: neural machine translation,\\nabstractive text summarization, and dialogue generation, under two generation\\nparadigms: fine-tuned small model and few-shot LLM. Our approach achieves\\nstate-of-the-art results in four directions in JRC-Acquis, XSum (50.3 ROUGE-1),\\nand BigPatent (62.9 ROUGE-1), demonstrating the potential of self-memory in\\nenhancing retrieval-augmented generation models. Furthermore, we conduct\\nthorough analyses of each component in the selfmem framework to identify\\nbottlenecks and provide insights for future research.'},\n",
       " {'published': datetime.date(2023, 3, 10),\n",
       "  'updated': datetime.date(2023, 3, 10),\n",
       "  'entry_id': 'http://arxiv.org/abs/2303.05692v1',\n",
       "  'Title': 'Semantic-Preserving Augmentation for Robust Image-Text Retrieval',\n",
       "  'Authors': ['Sunwoo Kim',\n",
       "   'Kyuhong Shim',\n",
       "   'Luong Trung Nguyen',\n",
       "   'Byonghyo Shim'],\n",
       "  'Summary': 'Image text retrieval is a task to search for the proper textual descriptions\\nof the visual world and vice versa. One challenge of this task is the\\nvulnerability to input image and text corruptions. Such corruptions are often\\nunobserved during the training, and degrade the retrieval model decision\\nquality substantially. In this paper, we propose a novel image text retrieval\\ntechnique, referred to as robust visual semantic embedding (RVSE), which\\nconsists of novel image-based and text-based augmentation techniques called\\nsemantic preserving augmentation for image (SPAugI) and text (SPAugT). Since\\nSPAugI and SPAugT change the original data in a way that its semantic\\ninformation is preserved, we enforce the feature extractors to generate\\nsemantic aware embedding vectors regardless of the corruption, improving the\\nmodel robustness significantly. From extensive experiments using benchmark\\ndatasets, we show that RVSE outperforms conventional retrieval schemes in terms\\nof image-text retrieval performance.'},\n",
       " {'published': datetime.date(2022, 8, 9),\n",
       "  'updated': datetime.date(2022, 8, 10),\n",
       "  'entry_id': 'http://arxiv.org/abs/2208.04887v2',\n",
       "  'Title': 'Early Stage Sparse Retrieval with Entity Linking',\n",
       "  'Authors': ['Dahlia Shehata', 'Negar Arabzadeh', 'Charles L. A. Clarke'],\n",
       "  'Summary': 'Despite the advantages of their low-resource settings, traditional sparse\\nretrievers depend on exact matching approaches between high-dimensional\\nbag-of-words (BoW) representations of both the queries and the collection. As a\\nresult, retrieval performance is restricted by semantic discrepancies and\\nvocabulary gaps. On the other hand, transformer-based dense retrievers\\nintroduce significant improvements in information retrieval tasks by exploiting\\nlow-dimensional contextualized representations of the corpus. While dense\\nretrievers are known for their relative effectiveness, they suffer from lower\\nefficiency and lack of generalization issues, when compared to sparse\\nretrievers. For a lightweight retrieval task, high computational resources and\\ntime consumption are major barriers encouraging the renunciation of dense\\nmodels despite potential gains. In this work, we propose boosting the\\nperformance of sparse retrievers by expanding both the queries and the\\ndocuments with linked entities in two formats for the entity names: 1) explicit\\nand 2) hashed. We employ a zero-shot end-to-end dense entity linking system for\\nentity recognition and disambiguation to augment the corpus. By leveraging the\\nadvanced entity linking methods, we believe that the effectiveness gap between\\nsparse and dense retrievers can be narrowed. We conduct our experiments on the\\nMS MARCO passage dataset. Since we are concerned with the early stage retrieval\\nin cascaded ranking architectures of large information retrieval systems, we\\nevaluate our results using recall@1000. Our approach is also capable of\\nretrieving documents for query subsets judged to be particularly difficult in\\nprior work. We further demonstrate that the non-expanded and the expanded runs\\nwith both explicit and hashed entities retrieve complementary results.\\nConsequently, we adopt a run fusion approach to maximize the benefits of entity\\nlinking.'},\n",
       " {'published': datetime.date(2023, 2, 11),\n",
       "  'updated': datetime.date(2023, 2, 14),\n",
       "  'entry_id': 'http://arxiv.org/abs/2302.05578v2',\n",
       "  'Title': 'Characterizing Attribution and Fluency Tradeoffs for Retrieval-Augmented Large Language Models',\n",
       "  'Authors': ['Renat Aksitov',\n",
       "   'Chung-Ching Chang',\n",
       "   'David Reitter',\n",
       "   'Siamak Shakeri',\n",
       "   'Yunhsuan Sung'],\n",
       "  'Summary': 'Despite recent progress, it has been difficult to prevent semantic\\nhallucinations in generative Large Language Models. One common solution to this\\nis augmenting LLMs with a retrieval system and making sure that the generated\\noutput is attributable to the retrieved information. Given this new added\\nconstraint, it is plausible to expect that the overall quality of the output\\nwill be affected, for example, in terms of fluency. Can scaling language models\\nhelp?\\n  Here we examine the relationship between fluency and attribution in LLMs\\nprompted with retrieved evidence in knowledge-heavy dialog settings. Our\\nexperiments were implemented with a set of auto-metrics that are aligned with\\nhuman preferences. They were used to evaluate a large set of generations,\\nproduced under varying parameters of LLMs and supplied context.\\n  We show that larger models tend to do much better in both fluency and\\nattribution, and that (naively) using top-k retrieval versus top-1 retrieval\\nimproves attribution but hurts fluency. We next propose a recipe that could\\nallow smaller models to both close the gap with larger models and preserve the\\nbenefits of top-k retrieval while avoiding its drawbacks.'},\n",
       " {'published': datetime.date(2023, 2, 16),\n",
       "  'updated': datetime.date(2023, 2, 16),\n",
       "  'entry_id': 'http://arxiv.org/abs/2302.08268v1',\n",
       "  'Title': 'Retrieval-augmented Image Captioning',\n",
       "  'Authors': ['Rita Ramos', 'Desmond Elliott', 'Bruno Martins'],\n",
       "  'Summary': 'Inspired by retrieval-augmented language generation and pretrained Vision and\\nLanguage (V&L) encoders, we present a new approach to image captioning that\\ngenerates sentences given the input image and a set of captions retrieved from\\na datastore, as opposed to the image alone. The encoder in our model jointly\\nprocesses the image and retrieved captions using a pretrained V&L BERT, while\\nthe decoder attends to the multimodal encoder representations, benefiting from\\nthe extra textual evidence from the retrieved captions. Experimental results on\\nthe COCO dataset show that image captioning can be effectively formulated from\\nthis new perspective. Our model, named EXTRA, benefits from using captions\\nretrieved from the training dataset, and it can also benefit from using an\\nexternal dataset without the need for retraining. Ablation studies show that\\nretrieving a sufficient number of captions (e.g., k=5) can improve captioning\\nquality. Our work contributes towards using pretrained V&L encoders for\\ngenerative tasks, instead of standard classification tasks.'},\n",
       " {'published': datetime.date(2023, 5, 18),\n",
       "  'updated': datetime.date(2023, 5, 18),\n",
       "  'entry_id': 'http://arxiv.org/abs/2305.10647v1',\n",
       "  'Title': 'BioAug: Conditional Generation based Data Augmentation for Low-Resource Biomedical NER',\n",
       "  'Authors': ['Sreyan Ghosh',\n",
       "   'Utkarsh Tyagi',\n",
       "   'Sonal Kumar',\n",
       "   'Dinesh Manocha'],\n",
       "  'Summary': 'Biomedical Named Entity Recognition (BioNER) is the fundamental task of\\nidentifying named entities from biomedical text. However, BioNER suffers from\\nsevere data scarcity and lacks high-quality labeled data due to the highly\\nspecialized and expert knowledge required for annotation. Though data\\naugmentation has shown to be highly effective for low-resource NER in general,\\nexisting data augmentation techniques fail to produce factual and diverse\\naugmentations for BioNER. In this paper, we present BioAug, a novel data\\naugmentation framework for low-resource BioNER. BioAug, built on BART, is\\ntrained to solve a novel text reconstruction task based on selective masking\\nand knowledge augmentation. Post training, we perform conditional generation\\nand generate diverse augmentations conditioning BioAug on selectively corrupted\\ntext similar to the training stage. We demonstrate the effectiveness of BioAug\\non 5 benchmark BioNER datasets and show that BioAug outperforms all our\\nbaselines by a significant margin (1.5%-21.5% absolute improvement) and is able\\nto generate augmentations that are both more factual and diverse. Code:\\nhttps://github.com/Sreyan88/BioAug.'},\n",
       " {'published': datetime.date(2022, 7, 28),\n",
       "  'updated': datetime.date(2022, 7, 28),\n",
       "  'entry_id': 'http://arxiv.org/abs/2207.13919v1',\n",
       "  'Title': 'Persona-Knowledge Dialogue Multi-Context Retrieval and Enhanced Decoding Methods',\n",
       "  'Authors': ['Min Sik Oh', 'Min Sang Kim'],\n",
       "  'Summary': 'Persona and Knowledge dual context open-domain chat is a novel dialogue\\ngeneration task introduced recently. While Persona and Knowledge is each\\ninteresting context of open-domain dialogue, the combination of both has not\\nbeen well studied. We tackle Persona-Knowledge identification and response\\ngeneration tasks in this paper. We design an informed data augmentation\\nstrategy that is compatible with neural Q&A retrieval models. With the\\naugmented data, we perform permutative Persona-Knowledge evaluation and\\nsuccessive Persona search fine-tuning. Furthermore, we perform dialogue\\ngeneration with various decoding techniques and illustrate crucial elements. We\\nachieve SOTA across official metrics with 93.99% Grounding accuracy average and\\n23.62 SacreBLEU score.'},\n",
       " {'published': datetime.date(2021, 10, 16),\n",
       "  'updated': datetime.date(2022, 2, 23),\n",
       "  'entry_id': 'http://arxiv.org/abs/2110.08458v2',\n",
       "  'Title': 'Controllable Semantic Parsing via Retrieval Augmentation',\n",
       "  'Authors': ['Panupong Pasupat', 'Yuan Zhang', 'Kelvin Guu'],\n",
       "  'Summary': 'In practical applications of semantic parsing, we often want to rapidly\\nchange the behavior of the parser, such as enabling it to handle queries in a\\nnew domain, or changing its predictions on certain targeted queries. While we\\ncan introduce new training examples exhibiting the target behavior, a mechanism\\nfor enacting such behavior changes without expensive model re-training would be\\npreferable. To this end, we propose ControllAble Semantic Parser via Exemplar\\nRetrieval (CASPER). Given an input query, the parser retrieves related\\nexemplars from a retrieval index, augments them to the query, and then applies\\na generative seq2seq model to produce an output parse. The exemplars act as a\\ncontrol mechanism over the generic generative model: by manipulating the\\nretrieval index or how the augmented query is constructed, we can manipulate\\nthe behavior of the parser. On the MTOP dataset, in addition to achieving\\nstate-of-the-art on the standard setup, we show that CASPER can parse queries\\nin a new domain, adapt the prediction toward the specified patterns, or adapt\\nto new semantic schemas without having to further re-train the model.'},\n",
       " {'published': datetime.date(2023, 4, 27),\n",
       "  'updated': datetime.date(2023, 4, 27),\n",
       "  'entry_id': 'http://arxiv.org/abs/2304.14233v1',\n",
       "  'Title': 'Large Language Models are Strong Zero-Shot Retriever',\n",
       "  'Authors': ['Tao Shen',\n",
       "   'Guodong Long',\n",
       "   'Xiubo Geng',\n",
       "   'Chongyang Tao',\n",
       "   'Tianyi Zhou',\n",
       "   'Daxin Jiang'],\n",
       "  'Summary': \"In this work, we propose a simple method that applies a large language model\\n(LLM) to large-scale retrieval in zero-shot scenarios. Our method, Language\\nlanguage model as Retriever (LameR) is built upon no other neural models but an\\nLLM, while breaking up brute-force combinations of retrievers with LLMs and\\nlifting the performance of zero-shot retrieval to be very competitive on\\nbenchmark datasets. Essentially, we propose to augment a query with its\\npotential answers by prompting LLMs with a composition of the query and the\\nquery's in-domain candidates. The candidates, regardless of correct or wrong,\\nare obtained by a vanilla retrieval procedure on the target collection. Such\\ncandidates, as a part of prompts, are likely to help LLM generate more precise\\nanswers by pattern imitation or candidate summarization. Even if all the\\ncandidates are wrong, the prompts at least make LLM aware of in-collection\\npatterns and genres. Moreover, due to the low performance of a self-supervised\\nretriever, the LLM-based query augmentation becomes less effective as the\\nretriever bottlenecks the whole pipeline. So, we propose to leverage a\\nnon-parametric lexicon-based method (e.g., BM25) as the retrieval module to\\ncapture query-document overlap in a literal fashion. As such, LameR makes the\\nretrieval procedure transparent to the LLM, so it circumvents the performance\\nbottleneck.\"},\n",
       " {'published': datetime.date(2023, 5, 6),\n",
       "  'updated': datetime.date(2023, 5, 6),\n",
       "  'entry_id': 'http://arxiv.org/abs/2305.03950v1',\n",
       "  'Title': 'Augmenting Passage Representations with Query Generation for Enhanced Cross-Lingual Dense Retrieval',\n",
       "  'Authors': ['Shengyao Zhuang', 'Linjun Shou', 'Guido Zuccon'],\n",
       "  'Summary': 'Effective cross-lingual dense retrieval methods that rely on multilingual\\npre-trained language models (PLMs) need to be trained to encompass both the\\nrelevance matching task and the cross-language alignment task. However,\\ncross-lingual data for training is often scarcely available. In this paper,\\nrather than using more cross-lingual data for training, we propose to use\\ncross-lingual query generation to augment passage representations with queries\\nin languages other than the original passage language. These augmented\\nrepresentations are used at inference time so that the representation can\\nencode more information across the different target languages. Training of a\\ncross-lingual query generator does not require additional training data to that\\nused for the dense retriever. The query generator training is also effective\\nbecause the pre-training task for the generator (T5 text-to-text training) is\\nvery similar to the fine-tuning task (generation of a query). The use of the\\ngenerator does not increase query latency at inference and can be combined with\\nany cross-lingual dense retrieval method. Results from experiments on a\\nbenchmark cross-lingual information retrieval dataset show that our approach\\ncan improve the effectiveness of existing cross-lingual dense retrieval\\nmethods. Implementation of our methods, along with all generated query files\\nare made publicly available at https://github.com/ielab/xQG4xDR.'},\n",
       " {'published': datetime.date(2023, 3, 1),\n",
       "  'updated': datetime.date(2023, 3, 1),\n",
       "  'entry_id': 'http://arxiv.org/abs/2303.00534v1',\n",
       "  'Title': 'RAMM: Retrieval-augmented Biomedical Visual Question Answering with Multi-modal Pre-training',\n",
       "  'Authors': ['Zheng Yuan',\n",
       "   'Qiao Jin',\n",
       "   'Chuanqi Tan',\n",
       "   'Zhengyun Zhao',\n",
       "   'Hongyi Yuan',\n",
       "   'Fei Huang',\n",
       "   'Songfang Huang'],\n",
       "  'Summary': 'Vision-and-language multi-modal pretraining and fine-tuning have shown great\\nsuccess in visual question answering (VQA). Compared to general domain VQA, the\\nperformance of biomedical VQA suffers from limited data. In this paper, we\\npropose a retrieval-augmented pretrain-and-finetune paradigm named RAMM for\\nbiomedical VQA to overcome the data limitation issue. Specifically, we collect\\na new biomedical dataset named PMCPM which offers patient-based image-text\\npairs containing diverse patient situations from PubMed. Then, we pretrain the\\nbiomedical multi-modal model to learn visual and textual representation for\\nimage-text pairs and align these representations with image-text contrastive\\nobjective (ITC). Finally, we propose a retrieval-augmented method to better use\\nthe limited data. We propose to retrieve similar image-text pairs based on ITC\\nfrom pretraining datasets and introduce a novel retrieval-attention module to\\nfuse the representation of the image and the question with the retrieved images\\nand texts. Experiments demonstrate that our retrieval-augmented\\npretrain-and-finetune paradigm obtains state-of-the-art performance on\\nMed-VQA2019, Med-VQA2021, VQARAD, and SLAKE datasets. Further analysis shows\\nthat the proposed RAMM and PMCPM can enhance biomedical VQA performance\\ncompared with previous resources and methods. We will open-source our dataset,\\ncodes, and pretrained model.'},\n",
       " {'published': datetime.date(2022, 4, 17),\n",
       "  'updated': datetime.date(2022, 10, 18),\n",
       "  'entry_id': 'http://arxiv.org/abs/2204.07937v2',\n",
       "  'Title': 'Unsupervised Cross-Task Generalization via Retrieval Augmentation',\n",
       "  'Authors': ['Bill Yuchen Lin',\n",
       "   'Kangmin Tan',\n",
       "   'Chris Miller',\n",
       "   'Beiwen Tian',\n",
       "   'Xiang Ren'],\n",
       "  'Summary': 'Humans can perform unseen tasks by recalling relevant skills acquired\\npreviously and then generalizing them to the target tasks, even if there is no\\nsupervision at all. In this paper, we aim to improve this kind of cross-task\\ngeneralization ability of massive multi-task language models, such as T0 and\\nFLAN, in an unsupervised setting. We propose a retrieval-augmentation method\\nnamed ReCross that takes a few unlabelled examples as queries to retrieve a\\nsmall subset of upstream data and uses them to update the multi-task model for\\nbetter generalization. ReCross is a straightforward yet effective retrieval\\nmethod that combines both efficient dense retrieval and effective pair-wise\\nreranking. Our results and analysis show that it significantly outperforms both\\nnon-retrieval methods and other baseline methods.'},\n",
       " {'published': datetime.date(2022, 10, 21),\n",
       "  'updated': datetime.date(2022, 10, 21),\n",
       "  'entry_id': 'http://arxiv.org/abs/2210.12285v1',\n",
       "  'Title': 'Exploring Representation-Level Augmentation for Code Search',\n",
       "  'Authors': ['Haochen Li',\n",
       "   'Chunyan Miao',\n",
       "   'Cyril Leung',\n",
       "   'Yanxian Huang',\n",
       "   'Yuan Huang',\n",
       "   'Hongyu Zhang',\n",
       "   'Yanlin Wang'],\n",
       "  'Summary': 'Code search, which aims at retrieving the most relevant code fragment for a\\ngiven natural language query, is a common activity in software development\\npractice. Recently, contrastive learning is widely used in code search\\nresearch, where many data augmentation approaches for source code (e.g.,\\nsemantic-preserving program transformation) are proposed to learn better\\nrepresentations. However, these augmentations are at the raw-data level, which\\nrequires additional code analysis in the preprocessing stage and additional\\ntraining costs in the training stage. In this paper, we explore augmentation\\nmethods that augment data (both code and query) at representation level which\\ndoes not require additional data processing and training, and based on this we\\npropose a general format of representation-level augmentation that unifies\\nexisting methods. Then, we propose three new augmentation methods (linear\\nextrapolation, binary interpolation, and Gaussian scaling) based on the general\\nformat. Furthermore, we theoretically analyze the advantages of the proposed\\naugmentation methods over traditional contrastive learning methods on code\\nsearch. We experimentally evaluate the proposed representation-level\\naugmentation methods with state-of-the-art code search models on a large-scale\\npublic dataset consisting of six programming languages. The experimental\\nresults show that our approach can consistently boost the performance of the\\nstudied code search models. Our source code is available at\\nhttps://github.com/Alex-HaochenLi/RACS.'},\n",
       " {'published': datetime.date(2022, 4, 21),\n",
       "  'updated': datetime.date(2022, 4, 21),\n",
       "  'entry_id': 'http://arxiv.org/abs/2205.00834v1',\n",
       "  'Title': 'Convex Augmentation for Total Variation Based Phase Retrieval',\n",
       "  'Authors': ['Jianwei Niu', 'Hok Shing Wong', 'Tieyong Zeng'],\n",
       "  'Summary': 'Phase retrieval is an important problem with significant physical and\\nindustrial applications. In this paper, we consider the case where the\\nmagnitude of the measurement of an underlying signal is corrupted by Gaussian\\nnoise. We introduce a convex augmentation approach for phase retrieval based on\\ntotal variation regularization. In contrast to popular convex relaxation models\\nlike PhaseLift, our model can be efficiently solved by a modified semi-proximal\\nalternating direction method of multipliers (sPADMM). The modified sPADMM is\\nmore general and flexible than the standard one, and its convergence is also\\nestablished in this paper. Extensive numerical experiments are conducted to\\nshowcase the effectiveness of the proposed method.'},\n",
       " {'published': datetime.date(2021, 12, 16),\n",
       "  'updated': datetime.date(2022, 7, 13),\n",
       "  'entry_id': 'http://arxiv.org/abs/2112.08816v2',\n",
       "  'Title': 'Deep Hash Distillation for Image Retrieval',\n",
       "  'Authors': ['Young Kyun Jang',\n",
       "   'Geonmo Gu',\n",
       "   'Byungsoo Ko',\n",
       "   'Isaac Kang',\n",
       "   'Nam Ik Cho'],\n",
       "  'Summary': 'In hash-based image retrieval systems, degraded or transformed inputs usually\\ngenerate different codes from the original, deteriorating the retrieval\\naccuracy. To mitigate this issue, data augmentation can be applied during\\ntraining. However, even if augmented samples of an image are similar in real\\nfeature space, the quantization can scatter them far away in Hamming space.\\nThis results in representation discrepancies that can impede training and\\ndegrade performance. In this work, we propose a novel self-distilled hashing\\nscheme to minimize the discrepancy while exploiting the potential of augmented\\ndata. By transferring the hash knowledge of the weakly-transformed samples to\\nthe strong ones, we make the hash code insensitive to various transformations.\\nWe also introduce hash proxy-based similarity learning and binary cross\\nentropy-based quantization loss to provide fine quality hash codes. Ultimately,\\nwe construct a deep hashing framework that not only improves the existing deep\\nhashing approaches, but also achieves the state-of-the-art retrieval results.\\nExtensive experiments are conducted and confirm the effectiveness of our work.'},\n",
       " {'published': datetime.date(2022, 2, 22),\n",
       "  'updated': datetime.date(2022, 2, 22),\n",
       "  'entry_id': 'http://arxiv.org/abs/2202.11233v1',\n",
       "  'Title': 'Retrieval Augmented Classification for Long-Tail Visual Recognition',\n",
       "  'Authors': ['Alexander Long',\n",
       "   'Wei Yin',\n",
       "   'Thalaiyasingam Ajanthan',\n",
       "   'Vu Nguyen',\n",
       "   'Pulak Purkait',\n",
       "   'Ravi Garg',\n",
       "   'Alan Blair',\n",
       "   'Chunhua Shen',\n",
       "   'Anton van den Hengel'],\n",
       "  'Summary': \"We introduce Retrieval Augmented Classification (RAC), a generic approach to\\naugmenting standard image classification pipelines with an explicit retrieval\\nmodule. RAC consists of a standard base image encoder fused with a parallel\\nretrieval branch that queries a non-parametric external memory of pre-encoded\\nimages and associated text snippets. We apply RAC to the problem of long-tail\\nclassification and demonstrate a significant improvement over previous\\nstate-of-the-art on Places365-LT and iNaturalist-2018 (14.5% and 6.7%\\nrespectively), despite using only the training datasets themselves as the\\nexternal information source. We demonstrate that RAC's retrieval module,\\nwithout prompting, learns a high level of accuracy on tail classes. This, in\\nturn, frees the base encoder to focus on common classes, and improve its\\nperformance thereon. RAC represents an alternative approach to utilizing large,\\npretrained models without requiring fine-tuning, as well as a first step\\ntowards more effectively making use of external memory within common computer\\nvision architectures.\"},\n",
       " {'published': datetime.date(2023, 4, 27),\n",
       "  'updated': datetime.date(2023, 4, 27),\n",
       "  'entry_id': 'http://arxiv.org/abs/2304.13923v1',\n",
       "  'Title': 'Retrieval-based Knowledge Augmented Vision Language Pre-training',\n",
       "  'Authors': ['Jiahua Rao',\n",
       "   'Zifei Shan',\n",
       "   'Longpo Liu',\n",
       "   'Yao Zhou',\n",
       "   'Yuedong Yang'],\n",
       "  'Summary': 'With recent progress in large-scale vision and language representation\\nlearning, Vision Language Pretraining (VLP) models have achieved promising\\nimprovements on various multi-modal downstream tasks. Albeit powerful, these\\npre-training models still do not take advantage of world knowledge, which is\\nimplicit in multi-modal data but comprises abundant and complementary\\ninformation. In this work, we propose a REtrieval-based knowledge Augmented\\nVision Language Pre-training model (REAVL), which retrieves world knowledge\\nfrom knowledge graphs (KGs) and incorporates them in vision-language\\npre-training. REAVL has two core components: a knowledge retriever that\\nretrieves knowledge given multi-modal data, and a knowledge-augmented model\\nthat fuses multi-modal data and knowledge. By novelly unifying four\\nknowledge-aware self-supervised tasks, REAVL promotes the mutual integration of\\nmulti-modal data and knowledge by fusing explicit knowledge with\\nvision-language pairs for masked multi-modal data modeling and KG relational\\nreasoning. Empirical experiments show that REAVL achieves new state-of-the-art\\nperformance uniformly on knowledge-based vision-language understanding and\\nmultimodal entity linking tasks, and competitive results on general\\nvision-language tasks while only using 0.2% pre-training data of the best\\nmodels.'},\n",
       " {'published': datetime.date(2023, 5, 30),\n",
       "  'updated': datetime.date(2023, 5, 30),\n",
       "  'entry_id': 'http://arxiv.org/abs/2305.18846v1',\n",
       "  'Title': 'Knowledge Graph-Augmented Language Models for Knowledge-Grounded Dialogue Generation',\n",
       "  'Authors': ['Minki Kang', 'Jin Myung Kwak', 'Jinheon Baek', 'Sung Ju Hwang'],\n",
       "  'Summary': 'Language models have achieved impressive performances on dialogue generation\\ntasks. However, when generating responses for a conversation that requires\\nfactual knowledge, they are far from perfect, due to an absence of mechanisms\\nto retrieve, encode, and reflect the knowledge in the generated responses. Some\\nknowledge-grounded dialogue generation methods tackle this problem by\\nleveraging facts from Knowledge Graphs (KGs); however, they do not guarantee\\nthat the model utilizes a relevant piece of knowledge from the KG. To overcome\\nthis limitation, we propose SUbgraph Retrieval-augmented GEneration (SURGE), a\\nframework for generating context-relevant and knowledge-grounded dialogues with\\nthe KG. Specifically, our SURGE framework first retrieves the relevant subgraph\\nfrom the KG, and then enforces consistency across facts by perturbing their\\nword embeddings conditioned by the retrieved subgraph. Then, we utilize\\ncontrastive learning to ensure that the generated texts have high similarity to\\nthe retrieved subgraphs. We validate our SURGE framework on OpendialKG and\\nKOMODIS datasets, showing that it generates high-quality dialogues that\\nfaithfully reflect the knowledge from KG.'},\n",
       " {'published': datetime.date(2023, 2, 15),\n",
       "  'updated': datetime.date(2023, 2, 15),\n",
       "  'entry_id': 'http://arxiv.org/abs/2302.07452v1',\n",
       "  'Title': 'How to Train Your DRAGON: Diverse Augmentation Towards Generalizable Dense Retrieval',\n",
       "  'Authors': ['Sheng-Chieh Lin',\n",
       "   'Akari Asai',\n",
       "   'Minghan Li',\n",
       "   'Barlas Oguz',\n",
       "   'Jimmy Lin',\n",
       "   'Yashar Mehdad',\n",
       "   'Wen-tau Yih',\n",
       "   'Xilun Chen'],\n",
       "  'Summary': 'Various techniques have been developed in recent years to improve dense\\nretrieval (DR), such as unsupervised contrastive learning and pseudo-query\\ngeneration. Existing DRs, however, often suffer from effectiveness tradeoffs\\nbetween supervised and zero-shot retrieval, which some argue was due to the\\nlimited model capacity. We contradict this hypothesis and show that a\\ngeneralizable DR can be trained to achieve high accuracy in both supervised and\\nzero-shot retrieval without increasing model size. In particular, we\\nsystematically examine the contrastive learning of DRs, under the framework of\\nData Augmentation (DA). Our study shows that common DA practices such as query\\naugmentation with generative models and pseudo-relevance label creation using a\\ncross-encoder, are often inefficient and sub-optimal. We hence propose a new DA\\napproach with diverse queries and sources of supervision to progressively train\\na generalizable DR. As a result, DRAGON, our dense retriever trained with\\ndiverse augmentation, is the first BERT-base-sized DR to achieve\\nstate-of-the-art effectiveness in both supervised and zero-shot evaluations and\\neven competes with models using more complex late interaction (ColBERTv2 and\\nSPLADE++).'},\n",
       " {'published': datetime.date(2023, 4, 20),\n",
       "  'updated': datetime.date(2023, 4, 20),\n",
       "  'entry_id': 'http://arxiv.org/abs/2304.10253v1',\n",
       "  'Title': 'A data augmentation perspective on diffusion models and retrieval',\n",
       "  'Authors': ['Max F. Burg',\n",
       "   'Florian Wenzel',\n",
       "   'Dominik Zietlow',\n",
       "   'Max Horn',\n",
       "   'Osama Makansi',\n",
       "   'Francesco Locatello',\n",
       "   'Chris Russell'],\n",
       "  'Summary': 'Diffusion models excel at generating photorealistic images from text-queries.\\nNaturally, many approaches have been proposed to use these generative abilities\\nto augment training datasets for downstream tasks, such as classification.\\nHowever, diffusion models are themselves trained on large noisily supervised,\\nbut nonetheless, annotated datasets. It is an open question whether the\\ngeneralization capabilities of diffusion models beyond using the additional\\ndata of the pre-training process for augmentation lead to improved downstream\\nperformance. We perform a systematic evaluation of existing methods to generate\\nimages from diffusion models and study new extensions to assess their benefit\\nfor data augmentation. While we find that personalizing diffusion models\\ntowards the target data outperforms simpler prompting strategies, we also show\\nthat using the training data of the diffusion model alone, via a simple nearest\\nneighbor retrieval procedure, leads to even stronger downstream performance.\\nOverall, our study probes the limitations of diffusion models for data\\naugmentation but also highlights its potential in generating new training data\\nto improve performance on simple downstream vision tasks.'},\n",
       " {'published': datetime.date(2022, 12, 17),\n",
       "  'updated': datetime.date(2023, 3, 7),\n",
       "  'entry_id': 'http://arxiv.org/abs/2212.08841v2',\n",
       "  'Title': 'AugTriever: Unsupervised Dense Retrieval by Scalable Data Augmentation',\n",
       "  'Authors': ['Rui Meng',\n",
       "   'Ye Liu',\n",
       "   'Semih Yavuz',\n",
       "   'Divyansh Agarwal',\n",
       "   'Lifu Tu',\n",
       "   'Ning Yu',\n",
       "   'Jianguo Zhang',\n",
       "   'Meghana Bhat',\n",
       "   'Yingbo Zhou'],\n",
       "  'Summary': 'Dense retrievers have made significant strides in text retrieval and\\nopen-domain question answering, even though most achievements were made\\npossible only with large amounts of human supervision. In this work, we aim to\\ndevelop unsupervised methods by proposing two methods that create pseudo\\nquery-document pairs and train dense retrieval models in an annotation-free and\\nscalable manner: query extraction and transferred query generation. The former\\nmethod produces pseudo queries by selecting salient spans from the original\\ndocument. The latter utilizes generation models trained for other NLP tasks\\n(e.g., summarization) to produce pseudo queries. Extensive experiments show\\nthat models trained with the proposed augmentation methods can perform\\ncomparably well (or better) to multiple strong baselines. Combining those\\nstrategies leads to further improvements, achieving the state-of-the-art\\nperformance of unsupervised dense retrieval on both BEIR and ODQA datasets.'},\n",
       " {'published': datetime.date(2022, 3, 5),\n",
       "  'updated': datetime.date(2022, 10, 22),\n",
       "  'entry_id': 'http://arxiv.org/abs/2203.02700v3',\n",
       "  'Title': 'RACE: Retrieval-Augmented Commit Message Generation',\n",
       "  'Authors': ['Ensheng Shi',\n",
       "   'Yanlin Wang',\n",
       "   'Wei Tao',\n",
       "   'Lun Du',\n",
       "   'Hongyu Zhang',\n",
       "   'Shi Han',\n",
       "   'Dongmei Zhang',\n",
       "   'Hongbin Sun'],\n",
       "  'Summary': 'Commit messages are important for software development and maintenance. Many\\nneural network-based approaches have been proposed and shown promising results\\non automatic commit message generation. However, the generated commit messages\\ncould be repetitive or redundant. In this paper, we propose RACE, a new\\nretrieval-augmented neural commit message generation method, which treats the\\nretrieved similar commit as an exemplar and leverages it to generate an\\naccurate commit message. As the retrieved commit message may not always\\naccurately describe the content/intent of the current code diff, we also\\npropose an exemplar guider, which learns the semantic similarity between the\\nretrieved and current code diff and then guides the generation of commit\\nmessage based on the similarity. We conduct extensive experiments on a large\\npublic dataset with five programming languages. Experimental results show that\\nRACE can outperform all baselines. Furthermore, RACE can boost the performance\\nof existing Seq2Seq models in commit message generation.'},\n",
       " {'published': datetime.date(2022, 10, 7),\n",
       "  'updated': datetime.date(2022, 10, 29),\n",
       "  'entry_id': 'http://arxiv.org/abs/2210.03809v2',\n",
       "  'Title': 'Retrieval Augmented Visual Question Answering with Outside Knowledge',\n",
       "  'Authors': ['Weizhe Lin', 'Bill Byrne'],\n",
       "  'Summary': 'Outside-Knowledge Visual Question Answering (OK-VQA) is a challenging VQA\\ntask that requires retrieval of external knowledge to answer questions about\\nimages. Recent OK-VQA systems use Dense Passage Retrieval (DPR) to retrieve\\ndocuments from external knowledge bases, such as Wikipedia, but with DPR\\ntrained separately from answer generation, introducing a potential limit on the\\noverall system performance. Instead, we propose a joint training scheme which\\nincludes differentiable DPR integrated with answer generation so that the\\nsystem can be trained in an end-to-end fashion. Our experiments show that our\\nscheme outperforms recent OK-VQA systems with strong DPR for retrieval. We also\\nintroduce new diagnostic metrics to analyze how retrieval and generation\\ninteract. The strong retrieval ability of our model significantly reduces the\\nnumber of retrieved documents needed in training, yielding significant benefits\\nin answer quality and computation required for training.'},\n",
       " {'published': datetime.date(2022, 9, 29),\n",
       "  'updated': datetime.date(2022, 9, 29),\n",
       "  'entry_id': 'http://arxiv.org/abs/2209.14899v1',\n",
       "  'Title': 'Generate-and-Retrieve: use your predictions to improve retrieval for semantic parsing',\n",
       "  'Authors': ['Yury Zemlyanskiy',\n",
       "   'Michiel de Jong',\n",
       "   'Joshua Ainslie',\n",
       "   'Panupong Pasupat',\n",
       "   'Peter Shaw',\n",
       "   'Linlu Qiu',\n",
       "   'Sumit Sanghai',\n",
       "   'Fei Sha'],\n",
       "  'Summary': 'A common recent approach to semantic parsing augments sequence-to-sequence\\nmodels by retrieving and appending a set of training samples, called exemplars.\\nThe effectiveness of this recipe is limited by the ability to retrieve\\ninformative exemplars that help produce the correct parse, which is especially\\nchallenging in low-resource settings. Existing retrieval is commonly based on\\nsimilarity of query and exemplar inputs. We propose GandR, a retrieval\\nprocedure that retrieves exemplars for which outputs are also similar.\\nGandRfirst generates a preliminary prediction with input-based retrieval. Then,\\nit retrieves exemplars with outputs similar to the preliminary prediction which\\nare used to generate a final prediction. GandR sets the state of the art on\\nmultiple low-resource semantic parsing tasks.'},\n",
       " {'published': datetime.date(2022, 5, 30),\n",
       "  'updated': datetime.date(2022, 5, 30),\n",
       "  'entry_id': 'http://arxiv.org/abs/2205.14981v1',\n",
       "  'Title': 'ZusammenQA: Data Augmentation with Specialized Models for Cross-lingual Open-retrieval Question Answering System',\n",
       "  'Authors': ['Chia-Chien Hung',\n",
       "   'Tommaso Green',\n",
       "   'Robert Litschko',\n",
       "   'Tornike Tsereteli',\n",
       "   'Sotaro Takeshita',\n",
       "   'Marco Bombieri',\n",
       "   'Goran Glavaš',\n",
       "   'Simone Paolo Ponzetto'],\n",
       "  'Summary': 'This paper introduces our proposed system for the MIA Shared Task on\\nCross-lingual Open-retrieval Question Answering (COQA). In this challenging\\nscenario, given an input question the system has to gather evidence documents\\nfrom a multilingual pool and generate from them an answer in the language of\\nthe question. We devised several approaches combining different model variants\\nfor three main components: Data Augmentation, Passage Retrieval, and Answer\\nGeneration. For passage retrieval, we evaluated the monolingual BM25 ranker\\nagainst the ensemble of re-rankers based on multilingual pretrained language\\nmodels (PLMs) and also variants of the shared task baseline, re-training it\\nfrom scratch using a recently introduced contrastive loss that maintains a\\nstrong gradient signal throughout training by means of mixed negative samples.\\nFor answer generation, we focused on language- and domain-specialization by\\nmeans of continued language model (LM) pretraining of existing multilingual\\nencoders. Additionally, for both passage retrieval and answer generation, we\\naugmented the training data provided by the task organizers with automatically\\ngenerated question-answer pairs created from Wikipedia passages to mitigate the\\nissue of data scarcity, particularly for the low-resource languages for which\\nno training data were provided. Our results show that language- and\\ndomain-specialization as well as data augmentation help, especially for\\nlow-resource languages.'},\n",
       " {'published': datetime.date(2022, 10, 10),\n",
       "  'updated': datetime.date(2022, 11, 1),\n",
       "  'entry_id': 'http://arxiv.org/abs/2210.04873v2',\n",
       "  'Title': 'CORE: A Retrieve-then-Edit Framework for Counterfactual Data Generation',\n",
       "  'Authors': ['Tanay Dixit',\n",
       "   'Bhargavi Paranjape',\n",
       "   'Hannaneh Hajishirzi',\n",
       "   'Luke Zettlemoyer'],\n",
       "  'Summary': 'Counterfactual data augmentation (CDA) -- i.e., adding minimally perturbed\\ninputs during training -- helps reduce model reliance on spurious correlations\\nand improves generalization to out-of-distribution (OOD) data. Prior work on\\ngenerating counterfactuals only considered restricted classes of perturbations,\\nlimiting their effectiveness. We present COunterfactual Generation via\\nRetrieval and Editing (CORE), a retrieval-augmented generation framework for\\ncreating diverse counterfactual perturbations for CDA. For each training\\nexample, CORE first performs a dense retrieval over a task-related unlabeled\\ntext corpus using a learned bi-encoder and extracts relevant counterfactual\\nexcerpts. CORE then incorporates these into prompts to a large language model\\nwith few-shot learning capabilities, for counterfactual editing. Conditioning\\nlanguage model edits on naturally occurring data results in diverse\\nperturbations. Experiments on natural language inference and sentiment analysis\\nbenchmarks show that CORE counterfactuals are more effective at improving\\ngeneralization to OOD data compared to other DA approaches. We also show that\\nthe CORE retrieval framework can be used to encourage diversity in manually\\nauthored perturbations'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cbeef3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\n",
    "    result\n",
    "    for result in arxiv.Search(  # type: ignore\n",
    "        query[: ARXIV_MAX_QUERY_LENGTH], max_results=top_k_results\n",
    "    ).results()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5eee6644",
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in docs[:4]:\n",
    "    statements = [do for do in d['Summary'].replace('\\n',' ').split('. ')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "242020fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rsch_prompts.memory_prompt import _DEFAULT_KNOWLEDGE_TRIPLE_EXTRACTION_TEMPLATE, _DEFAULT_ENTITY_EXTRACTION_TEMPLATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c16c6537",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./2305.14975v1.Just_Ask_for_Calibration_Strategies_for_Eliciting_Calibrated_Confidence_Scores_from_Language_Models_Fine_Tuned_with_Human_Feedback/emnlp2023.tex','r') as f:\n",
    "    ls = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e16a2b92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.\\n'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ls[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c4879fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "statements = '\\n'.join([f\"Statement: {s}\" for s in statements])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9b0ae049",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>published</th>\n",
       "      <th>updated</th>\n",
       "      <th>entry_id</th>\n",
       "      <th>Title</th>\n",
       "      <th>Authors</th>\n",
       "      <th>Summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-02-02</td>\n",
       "      <td>2022-02-13</td>\n",
       "      <td>http://arxiv.org/abs/2202.01110v2</td>\n",
       "      <td>A Survey on Retrieval-Augmented Text Generation</td>\n",
       "      <td>[Huayang Li, Yixuan Su, Deng Cai, Yan Wang, Le...</td>\n",
       "      <td>Recently, retrieval-augmented text generation ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-05-11</td>\n",
       "      <td>2023-05-11</td>\n",
       "      <td>http://arxiv.org/abs/2305.06983v1</td>\n",
       "      <td>Active Retrieval Augmented Generation</td>\n",
       "      <td>[Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqin...</td>\n",
       "      <td>Despite the remarkable ability of large langua...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-05-27</td>\n",
       "      <td>2023-05-27</td>\n",
       "      <td>http://arxiv.org/abs/2305.17331v1</td>\n",
       "      <td>Augmentation-Adapted Retriever Improves Genera...</td>\n",
       "      <td>[Zichun Yu, Chenyan Xiong, Shi Yu, Zhiyuan Liu]</td>\n",
       "      <td>Retrieval augmentation can aid language models...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-02-07</td>\n",
       "      <td>2023-02-07</td>\n",
       "      <td>http://arxiv.org/abs/2302.03754v1</td>\n",
       "      <td>Augmenting Zero-Shot Dense Retrievers with Plu...</td>\n",
       "      <td>[Suyu Ge, Chenyan Xiong, Corby Rosset, Arnold ...</td>\n",
       "      <td>In this paper we improve the zero-shot general...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-12-18</td>\n",
       "      <td>2023-05-07</td>\n",
       "      <td>http://arxiv.org/abs/2212.09146v2</td>\n",
       "      <td>Can Retriever-Augmented Language Models Reason...</td>\n",
       "      <td>[Parishad BehnamGhader, Santiago Miret, Siva R...</td>\n",
       "      <td>Augmenting pretrained language models with ret...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2023-05-24</td>\n",
       "      <td>2023-05-24</td>\n",
       "      <td>http://arxiv.org/abs/2305.15294v1</td>\n",
       "      <td>Enhancing Retrieval-Augmented Large Language M...</td>\n",
       "      <td>[Zhihong Shao, Yeyun Gong, Yelong Shen, Minlie...</td>\n",
       "      <td>Large language models are powerful text proces...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2022-07-29</td>\n",
       "      <td>2022-07-29</td>\n",
       "      <td>http://arxiv.org/abs/2207.14428v1</td>\n",
       "      <td>Paired Cross-Modal Data Augmentation for Fine-...</td>\n",
       "      <td>[Hao Wang, Guosheng Lin, Steven C. H. Hoi, Chu...</td>\n",
       "      <td>This paper investigates an open research probl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2023-02-22</td>\n",
       "      <td>2023-02-22</td>\n",
       "      <td>http://arxiv.org/abs/2302.11352v1</td>\n",
       "      <td>X-TRA: Improving Chest X-ray Tasks with Cross-...</td>\n",
       "      <td>[Tom van Sonsbeek, Marcel Worring]</td>\n",
       "      <td>An important component of human analysis of me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2023-06-08</td>\n",
       "      <td>2023-06-08</td>\n",
       "      <td>http://arxiv.org/abs/2306.05212v1</td>\n",
       "      <td>RETA-LLM: A Retrieval-Augmented Large Language...</td>\n",
       "      <td>[Jiongnan Liu, Jiajie Jin, Zihan Wang, Jiehan ...</td>\n",
       "      <td>Although Large Language Models (LLMs) have dem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2022-05-21</td>\n",
       "      <td>2022-06-01</td>\n",
       "      <td>http://arxiv.org/abs/2205.10471v2</td>\n",
       "      <td>Retrieval-Augmented Multilingual Keyphrase Gen...</td>\n",
       "      <td>[Yifan Gao, Qingyu Yin, Zheng Li, Rui Meng, To...</td>\n",
       "      <td>Keyphrase generation is the task of automatica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2021-12-10</td>\n",
       "      <td>2021-12-10</td>\n",
       "      <td>http://arxiv.org/abs/2112.07618v1</td>\n",
       "      <td>Robust Information Retrieval for False Claims ...</td>\n",
       "      <td>[Mingwen Dong, Christos Christodoulopoulos, Sh...</td>\n",
       "      <td>Accurate evidence retrieval is essential for a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2023-06-23</td>\n",
       "      <td>2023-06-23</td>\n",
       "      <td>http://arxiv.org/abs/2306.13421v1</td>\n",
       "      <td>Long-range Language Modeling with Self-retrieval</td>\n",
       "      <td>[Ohad Rubin, Jonathan Berant]</td>\n",
       "      <td>Retrieval-augmented language models (LMs) have...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2023-05-28</td>\n",
       "      <td>2023-05-28</td>\n",
       "      <td>http://arxiv.org/abs/2305.17653v1</td>\n",
       "      <td>Prompt-Guided Retrieval Augmentation for Non-K...</td>\n",
       "      <td>[Zhicheng Guo, Sijie Cheng, Yile Wang, Peng Li...</td>\n",
       "      <td>Retrieval-augmented methods have received incr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2022-10-23</td>\n",
       "      <td>2022-10-23</td>\n",
       "      <td>http://arxiv.org/abs/2210.12887v1</td>\n",
       "      <td>Retrieval Augmentation for Commonsense Reasoni...</td>\n",
       "      <td>[Wenhao Yu, Chenguang Zhu, Zhihan Zhang, Shuoh...</td>\n",
       "      <td>A common thread of retrieval-augmented methods...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2021-08-26</td>\n",
       "      <td>2021-09-10</td>\n",
       "      <td>http://arxiv.org/abs/2108.11601v2</td>\n",
       "      <td>Retrieval Augmented Code Generation and Summar...</td>\n",
       "      <td>[Md Rizwan Parvez, Wasi Uddin Ahmad, Saikat Ch...</td>\n",
       "      <td>Software developers write a lot of source code...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2023-07-07</td>\n",
       "      <td>2023-07-07</td>\n",
       "      <td>http://arxiv.org/abs/2307.04642v1</td>\n",
       "      <td>TRAC: Trustworthy Retrieval Augmented Chatbot</td>\n",
       "      <td>[Shuo Li, Sangdon Park, Insup Lee, Osbert Bast...</td>\n",
       "      <td>Although conversational AIs have demonstrated ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2021-04-15</td>\n",
       "      <td>2022-01-21</td>\n",
       "      <td>http://arxiv.org/abs/2104.07713v2</td>\n",
       "      <td>Contrastive Learning with Stronger Augmentations</td>\n",
       "      <td>[Xiao Wang, Guo-Jun Qi]</td>\n",
       "      <td>Representation learning has significantly been...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2022-12-20</td>\n",
       "      <td>2022-12-20</td>\n",
       "      <td>http://arxiv.org/abs/2212.10692v1</td>\n",
       "      <td>Generation-Augmented Query Expansion For Code ...</td>\n",
       "      <td>[Dong Li, Yelong Shen, Ruoming Jin, Yi Mao, Ku...</td>\n",
       "      <td>Pre-trained language models have achieved prom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2023-05-18</td>\n",
       "      <td>2023-05-18</td>\n",
       "      <td>http://arxiv.org/abs/2305.11074v1</td>\n",
       "      <td>Tram: A Token-level Retrieval-augmented Mechan...</td>\n",
       "      <td>[Tong Ye, Lingfei Wu, Tengfei Ma, Xuhong Zhang...</td>\n",
       "      <td>Automatically generating human-readable text d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2023-03-20</td>\n",
       "      <td>2023-03-20</td>\n",
       "      <td>http://arxiv.org/abs/2303.10868v1</td>\n",
       "      <td>Retrieving Multimodal Information for Augmente...</td>\n",
       "      <td>[Ruochen Zhao, Hailin Chen, Weishi Wang, Fangk...</td>\n",
       "      <td>In this survey, we review methods that retriev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2020-09-17</td>\n",
       "      <td>2021-08-06</td>\n",
       "      <td>http://arxiv.org/abs/2009.08553v4</td>\n",
       "      <td>Generation-Augmented Retrieval for Open-domain...</td>\n",
       "      <td>[Yuning Mao, Pengcheng He, Xiaodong Liu, Yelon...</td>\n",
       "      <td>We propose Generation-Augmented Retrieval (GAR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2023-05-05</td>\n",
       "      <td>2023-05-05</td>\n",
       "      <td>http://arxiv.org/abs/2305.03660v1</td>\n",
       "      <td>Retrieval Augmented Chest X-Ray Report Generat...</td>\n",
       "      <td>[Mercy Ranjit, Gopinath Ganapathy, Ranjit Manu...</td>\n",
       "      <td>We propose Retrieval Augmented Generation (RAG...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2022-09-28</td>\n",
       "      <td>2022-09-28</td>\n",
       "      <td>http://arxiv.org/abs/2209.14290v1</td>\n",
       "      <td>FiD-Light: Efficient and Effective Retrieval-A...</td>\n",
       "      <td>[Sebastian Hofstätter, Jiecao Chen, Karthik Ra...</td>\n",
       "      <td>Retrieval-augmented generation models offer ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2023-04-13</td>\n",
       "      <td>2023-04-13</td>\n",
       "      <td>http://arxiv.org/abs/2304.06762v1</td>\n",
       "      <td>Shall We Pretrain Autoregressive Language Mode...</td>\n",
       "      <td>[Boxin Wang, Wei Ping, Peng Xu, Lawrence McAfe...</td>\n",
       "      <td>Large decoder-only language models (LMs) can b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2023-05-03</td>\n",
       "      <td>2023-05-17</td>\n",
       "      <td>http://arxiv.org/abs/2305.02437v2</td>\n",
       "      <td>Lift Yourself Up: Retrieval-augmented Text Gen...</td>\n",
       "      <td>[Xin Cheng, Di Luo, Xiuying Chen, Lemao Liu, D...</td>\n",
       "      <td>With direct access to human-written reference ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2023-03-10</td>\n",
       "      <td>2023-03-10</td>\n",
       "      <td>http://arxiv.org/abs/2303.05692v1</td>\n",
       "      <td>Semantic-Preserving Augmentation for Robust Im...</td>\n",
       "      <td>[Sunwoo Kim, Kyuhong Shim, Luong Trung Nguyen,...</td>\n",
       "      <td>Image text retrieval is a task to search for t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2022-08-09</td>\n",
       "      <td>2022-08-10</td>\n",
       "      <td>http://arxiv.org/abs/2208.04887v2</td>\n",
       "      <td>Early Stage Sparse Retrieval with Entity Linking</td>\n",
       "      <td>[Dahlia Shehata, Negar Arabzadeh, Charles L. A...</td>\n",
       "      <td>Despite the advantages of their low-resource s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2023-02-16</td>\n",
       "      <td>2023-02-16</td>\n",
       "      <td>http://arxiv.org/abs/2302.08268v1</td>\n",
       "      <td>Retrieval-augmented Image Captioning</td>\n",
       "      <td>[Rita Ramos, Desmond Elliott, Bruno Martins]</td>\n",
       "      <td>Inspired by retrieval-augmented language gener...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2023-02-11</td>\n",
       "      <td>2023-02-14</td>\n",
       "      <td>http://arxiv.org/abs/2302.05578v2</td>\n",
       "      <td>Characterizing Attribution and Fluency Tradeof...</td>\n",
       "      <td>[Renat Aksitov, Chung-Ching Chang, David Reitt...</td>\n",
       "      <td>Despite recent progress, it has been difficult...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2023-05-18</td>\n",
       "      <td>2023-05-18</td>\n",
       "      <td>http://arxiv.org/abs/2305.10647v1</td>\n",
       "      <td>BioAug: Conditional Generation based Data Augm...</td>\n",
       "      <td>[Sreyan Ghosh, Utkarsh Tyagi, Sonal Kumar, Din...</td>\n",
       "      <td>Biomedical Named Entity Recognition (BioNER) i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2022-07-28</td>\n",
       "      <td>2022-07-28</td>\n",
       "      <td>http://arxiv.org/abs/2207.13919v1</td>\n",
       "      <td>Persona-Knowledge Dialogue Multi-Context Retri...</td>\n",
       "      <td>[Min Sik Oh, Min Sang Kim]</td>\n",
       "      <td>Persona and Knowledge dual context open-domain...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2021-10-16</td>\n",
       "      <td>2022-02-23</td>\n",
       "      <td>http://arxiv.org/abs/2110.08458v2</td>\n",
       "      <td>Controllable Semantic Parsing via Retrieval Au...</td>\n",
       "      <td>[Panupong Pasupat, Yuan Zhang, Kelvin Guu]</td>\n",
       "      <td>In practical applications of semantic parsing,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2023-04-27</td>\n",
       "      <td>2023-04-27</td>\n",
       "      <td>http://arxiv.org/abs/2304.14233v1</td>\n",
       "      <td>Large Language Models are Strong Zero-Shot Ret...</td>\n",
       "      <td>[Tao Shen, Guodong Long, Xiubo Geng, Chongyang...</td>\n",
       "      <td>In this work, we propose a simple method that ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2023-05-06</td>\n",
       "      <td>2023-05-06</td>\n",
       "      <td>http://arxiv.org/abs/2305.03950v1</td>\n",
       "      <td>Augmenting Passage Representations with Query ...</td>\n",
       "      <td>[Shengyao Zhuang, Linjun Shou, Guido Zuccon]</td>\n",
       "      <td>Effective cross-lingual dense retrieval method...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2023-03-01</td>\n",
       "      <td>2023-03-01</td>\n",
       "      <td>http://arxiv.org/abs/2303.00534v1</td>\n",
       "      <td>RAMM: Retrieval-augmented Biomedical Visual Qu...</td>\n",
       "      <td>[Zheng Yuan, Qiao Jin, Chuanqi Tan, Zhengyun Z...</td>\n",
       "      <td>Vision-and-language multi-modal pretraining an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2022-04-17</td>\n",
       "      <td>2022-10-18</td>\n",
       "      <td>http://arxiv.org/abs/2204.07937v2</td>\n",
       "      <td>Unsupervised Cross-Task Generalization via Ret...</td>\n",
       "      <td>[Bill Yuchen Lin, Kangmin Tan, Chris Miller, B...</td>\n",
       "      <td>Humans can perform unseen tasks by recalling r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2022-10-21</td>\n",
       "      <td>2022-10-21</td>\n",
       "      <td>http://arxiv.org/abs/2210.12285v1</td>\n",
       "      <td>Exploring Representation-Level Augmentation fo...</td>\n",
       "      <td>[Haochen Li, Chunyan Miao, Cyril Leung, Yanxia...</td>\n",
       "      <td>Code search, which aims at retrieving the most...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2022-04-21</td>\n",
       "      <td>2022-04-21</td>\n",
       "      <td>http://arxiv.org/abs/2205.00834v1</td>\n",
       "      <td>Convex Augmentation for Total Variation Based ...</td>\n",
       "      <td>[Jianwei Niu, Hok Shing Wong, Tieyong Zeng]</td>\n",
       "      <td>Phase retrieval is an important problem with s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2021-12-16</td>\n",
       "      <td>2022-07-13</td>\n",
       "      <td>http://arxiv.org/abs/2112.08816v2</td>\n",
       "      <td>Deep Hash Distillation for Image Retrieval</td>\n",
       "      <td>[Young Kyun Jang, Geonmo Gu, Byungsoo Ko, Isaa...</td>\n",
       "      <td>In hash-based image retrieval systems, degrade...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>2022-02-22</td>\n",
       "      <td>2022-02-22</td>\n",
       "      <td>http://arxiv.org/abs/2202.11233v1</td>\n",
       "      <td>Retrieval Augmented Classification for Long-Ta...</td>\n",
       "      <td>[Alexander Long, Wei Yin, Thalaiyasingam Ajant...</td>\n",
       "      <td>We introduce Retrieval Augmented Classificatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>2023-04-27</td>\n",
       "      <td>2023-04-27</td>\n",
       "      <td>http://arxiv.org/abs/2304.13923v1</td>\n",
       "      <td>Retrieval-based Knowledge Augmented Vision Lan...</td>\n",
       "      <td>[Jiahua Rao, Zifei Shan, Longpo Liu, Yao Zhou,...</td>\n",
       "      <td>With recent progress in large-scale vision and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>2023-05-30</td>\n",
       "      <td>2023-05-30</td>\n",
       "      <td>http://arxiv.org/abs/2305.18846v1</td>\n",
       "      <td>Knowledge Graph-Augmented Language Models for ...</td>\n",
       "      <td>[Minki Kang, Jin Myung Kwak, Jinheon Baek, Sun...</td>\n",
       "      <td>Language models have achieved impressive perfo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>2023-02-15</td>\n",
       "      <td>2023-02-15</td>\n",
       "      <td>http://arxiv.org/abs/2302.07452v1</td>\n",
       "      <td>How to Train Your DRAGON: Diverse Augmentation...</td>\n",
       "      <td>[Sheng-Chieh Lin, Akari Asai, Minghan Li, Barl...</td>\n",
       "      <td>Various techniques have been developed in rece...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>2023-04-20</td>\n",
       "      <td>2023-04-20</td>\n",
       "      <td>http://arxiv.org/abs/2304.10253v1</td>\n",
       "      <td>A data augmentation perspective on diffusion m...</td>\n",
       "      <td>[Max F. Burg, Florian Wenzel, Dominik Zietlow,...</td>\n",
       "      <td>Diffusion models excel at generating photoreal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>2022-12-17</td>\n",
       "      <td>2023-03-07</td>\n",
       "      <td>http://arxiv.org/abs/2212.08841v2</td>\n",
       "      <td>AugTriever: Unsupervised Dense Retrieval by Sc...</td>\n",
       "      <td>[Rui Meng, Ye Liu, Semih Yavuz, Divyansh Agarw...</td>\n",
       "      <td>Dense retrievers have made significant strides...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>2022-03-05</td>\n",
       "      <td>2022-10-22</td>\n",
       "      <td>http://arxiv.org/abs/2203.02700v3</td>\n",
       "      <td>RACE: Retrieval-Augmented Commit Message Gener...</td>\n",
       "      <td>[Ensheng Shi, Yanlin Wang, Wei Tao, Lun Du, Ho...</td>\n",
       "      <td>Commit messages are important for software dev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>2022-09-29</td>\n",
       "      <td>2022-09-29</td>\n",
       "      <td>http://arxiv.org/abs/2209.14899v1</td>\n",
       "      <td>Generate-and-Retrieve: use your predictions to...</td>\n",
       "      <td>[Yury Zemlyanskiy, Michiel de Jong, Joshua Ain...</td>\n",
       "      <td>A common recent approach to semantic parsing a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>2022-10-07</td>\n",
       "      <td>2022-10-29</td>\n",
       "      <td>http://arxiv.org/abs/2210.03809v2</td>\n",
       "      <td>Retrieval Augmented Visual Question Answering ...</td>\n",
       "      <td>[Weizhe Lin, Bill Byrne]</td>\n",
       "      <td>Outside-Knowledge Visual Question Answering (O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>2022-05-30</td>\n",
       "      <td>2022-05-30</td>\n",
       "      <td>http://arxiv.org/abs/2205.14981v1</td>\n",
       "      <td>ZusammenQA: Data Augmentation with Specialized...</td>\n",
       "      <td>[Chia-Chien Hung, Tommaso Green, Robert Litsch...</td>\n",
       "      <td>This paper introduces our proposed system for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>2022-10-10</td>\n",
       "      <td>2022-11-01</td>\n",
       "      <td>http://arxiv.org/abs/2210.04873v2</td>\n",
       "      <td>CORE: A Retrieve-then-Edit Framework for Count...</td>\n",
       "      <td>[Tanay Dixit, Bhargavi Paranjape, Hannaneh Haj...</td>\n",
       "      <td>Counterfactual data augmentation (CDA) -- i.e....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     published     updated                           entry_id   \n",
       "0   2022-02-02  2022-02-13  http://arxiv.org/abs/2202.01110v2  \\\n",
       "1   2023-05-11  2023-05-11  http://arxiv.org/abs/2305.06983v1   \n",
       "2   2023-05-27  2023-05-27  http://arxiv.org/abs/2305.17331v1   \n",
       "3   2023-02-07  2023-02-07  http://arxiv.org/abs/2302.03754v1   \n",
       "4   2022-12-18  2023-05-07  http://arxiv.org/abs/2212.09146v2   \n",
       "5   2023-05-24  2023-05-24  http://arxiv.org/abs/2305.15294v1   \n",
       "6   2022-07-29  2022-07-29  http://arxiv.org/abs/2207.14428v1   \n",
       "7   2023-02-22  2023-02-22  http://arxiv.org/abs/2302.11352v1   \n",
       "8   2023-06-08  2023-06-08  http://arxiv.org/abs/2306.05212v1   \n",
       "9   2022-05-21  2022-06-01  http://arxiv.org/abs/2205.10471v2   \n",
       "10  2021-12-10  2021-12-10  http://arxiv.org/abs/2112.07618v1   \n",
       "11  2023-06-23  2023-06-23  http://arxiv.org/abs/2306.13421v1   \n",
       "12  2023-05-28  2023-05-28  http://arxiv.org/abs/2305.17653v1   \n",
       "13  2022-10-23  2022-10-23  http://arxiv.org/abs/2210.12887v1   \n",
       "14  2021-08-26  2021-09-10  http://arxiv.org/abs/2108.11601v2   \n",
       "15  2023-07-07  2023-07-07  http://arxiv.org/abs/2307.04642v1   \n",
       "16  2021-04-15  2022-01-21  http://arxiv.org/abs/2104.07713v2   \n",
       "17  2022-12-20  2022-12-20  http://arxiv.org/abs/2212.10692v1   \n",
       "18  2023-05-18  2023-05-18  http://arxiv.org/abs/2305.11074v1   \n",
       "19  2023-03-20  2023-03-20  http://arxiv.org/abs/2303.10868v1   \n",
       "20  2020-09-17  2021-08-06  http://arxiv.org/abs/2009.08553v4   \n",
       "21  2023-05-05  2023-05-05  http://arxiv.org/abs/2305.03660v1   \n",
       "22  2022-09-28  2022-09-28  http://arxiv.org/abs/2209.14290v1   \n",
       "23  2023-04-13  2023-04-13  http://arxiv.org/abs/2304.06762v1   \n",
       "24  2023-05-03  2023-05-17  http://arxiv.org/abs/2305.02437v2   \n",
       "25  2023-03-10  2023-03-10  http://arxiv.org/abs/2303.05692v1   \n",
       "26  2022-08-09  2022-08-10  http://arxiv.org/abs/2208.04887v2   \n",
       "27  2023-02-16  2023-02-16  http://arxiv.org/abs/2302.08268v1   \n",
       "28  2023-02-11  2023-02-14  http://arxiv.org/abs/2302.05578v2   \n",
       "29  2023-05-18  2023-05-18  http://arxiv.org/abs/2305.10647v1   \n",
       "30  2022-07-28  2022-07-28  http://arxiv.org/abs/2207.13919v1   \n",
       "31  2021-10-16  2022-02-23  http://arxiv.org/abs/2110.08458v2   \n",
       "32  2023-04-27  2023-04-27  http://arxiv.org/abs/2304.14233v1   \n",
       "33  2023-05-06  2023-05-06  http://arxiv.org/abs/2305.03950v1   \n",
       "34  2023-03-01  2023-03-01  http://arxiv.org/abs/2303.00534v1   \n",
       "35  2022-04-17  2022-10-18  http://arxiv.org/abs/2204.07937v2   \n",
       "36  2022-10-21  2022-10-21  http://arxiv.org/abs/2210.12285v1   \n",
       "37  2022-04-21  2022-04-21  http://arxiv.org/abs/2205.00834v1   \n",
       "38  2021-12-16  2022-07-13  http://arxiv.org/abs/2112.08816v2   \n",
       "39  2022-02-22  2022-02-22  http://arxiv.org/abs/2202.11233v1   \n",
       "40  2023-04-27  2023-04-27  http://arxiv.org/abs/2304.13923v1   \n",
       "41  2023-05-30  2023-05-30  http://arxiv.org/abs/2305.18846v1   \n",
       "42  2023-02-15  2023-02-15  http://arxiv.org/abs/2302.07452v1   \n",
       "43  2023-04-20  2023-04-20  http://arxiv.org/abs/2304.10253v1   \n",
       "44  2022-12-17  2023-03-07  http://arxiv.org/abs/2212.08841v2   \n",
       "45  2022-03-05  2022-10-22  http://arxiv.org/abs/2203.02700v3   \n",
       "46  2022-09-29  2022-09-29  http://arxiv.org/abs/2209.14899v1   \n",
       "47  2022-10-07  2022-10-29  http://arxiv.org/abs/2210.03809v2   \n",
       "48  2022-05-30  2022-05-30  http://arxiv.org/abs/2205.14981v1   \n",
       "49  2022-10-10  2022-11-01  http://arxiv.org/abs/2210.04873v2   \n",
       "\n",
       "                                                Title   \n",
       "0     A Survey on Retrieval-Augmented Text Generation  \\\n",
       "1               Active Retrieval Augmented Generation   \n",
       "2   Augmentation-Adapted Retriever Improves Genera...   \n",
       "3   Augmenting Zero-Shot Dense Retrievers with Plu...   \n",
       "4   Can Retriever-Augmented Language Models Reason...   \n",
       "5   Enhancing Retrieval-Augmented Large Language M...   \n",
       "6   Paired Cross-Modal Data Augmentation for Fine-...   \n",
       "7   X-TRA: Improving Chest X-ray Tasks with Cross-...   \n",
       "8   RETA-LLM: A Retrieval-Augmented Large Language...   \n",
       "9   Retrieval-Augmented Multilingual Keyphrase Gen...   \n",
       "10  Robust Information Retrieval for False Claims ...   \n",
       "11   Long-range Language Modeling with Self-retrieval   \n",
       "12  Prompt-Guided Retrieval Augmentation for Non-K...   \n",
       "13  Retrieval Augmentation for Commonsense Reasoni...   \n",
       "14  Retrieval Augmented Code Generation and Summar...   \n",
       "15      TRAC: Trustworthy Retrieval Augmented Chatbot   \n",
       "16   Contrastive Learning with Stronger Augmentations   \n",
       "17  Generation-Augmented Query Expansion For Code ...   \n",
       "18  Tram: A Token-level Retrieval-augmented Mechan...   \n",
       "19  Retrieving Multimodal Information for Augmente...   \n",
       "20  Generation-Augmented Retrieval for Open-domain...   \n",
       "21  Retrieval Augmented Chest X-Ray Report Generat...   \n",
       "22  FiD-Light: Efficient and Effective Retrieval-A...   \n",
       "23  Shall We Pretrain Autoregressive Language Mode...   \n",
       "24  Lift Yourself Up: Retrieval-augmented Text Gen...   \n",
       "25  Semantic-Preserving Augmentation for Robust Im...   \n",
       "26   Early Stage Sparse Retrieval with Entity Linking   \n",
       "27               Retrieval-augmented Image Captioning   \n",
       "28  Characterizing Attribution and Fluency Tradeof...   \n",
       "29  BioAug: Conditional Generation based Data Augm...   \n",
       "30  Persona-Knowledge Dialogue Multi-Context Retri...   \n",
       "31  Controllable Semantic Parsing via Retrieval Au...   \n",
       "32  Large Language Models are Strong Zero-Shot Ret...   \n",
       "33  Augmenting Passage Representations with Query ...   \n",
       "34  RAMM: Retrieval-augmented Biomedical Visual Qu...   \n",
       "35  Unsupervised Cross-Task Generalization via Ret...   \n",
       "36  Exploring Representation-Level Augmentation fo...   \n",
       "37  Convex Augmentation for Total Variation Based ...   \n",
       "38         Deep Hash Distillation for Image Retrieval   \n",
       "39  Retrieval Augmented Classification for Long-Ta...   \n",
       "40  Retrieval-based Knowledge Augmented Vision Lan...   \n",
       "41  Knowledge Graph-Augmented Language Models for ...   \n",
       "42  How to Train Your DRAGON: Diverse Augmentation...   \n",
       "43  A data augmentation perspective on diffusion m...   \n",
       "44  AugTriever: Unsupervised Dense Retrieval by Sc...   \n",
       "45  RACE: Retrieval-Augmented Commit Message Gener...   \n",
       "46  Generate-and-Retrieve: use your predictions to...   \n",
       "47  Retrieval Augmented Visual Question Answering ...   \n",
       "48  ZusammenQA: Data Augmentation with Specialized...   \n",
       "49  CORE: A Retrieve-then-Edit Framework for Count...   \n",
       "\n",
       "                                              Authors   \n",
       "0   [Huayang Li, Yixuan Su, Deng Cai, Yan Wang, Le...  \\\n",
       "1   [Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqin...   \n",
       "2     [Zichun Yu, Chenyan Xiong, Shi Yu, Zhiyuan Liu]   \n",
       "3   [Suyu Ge, Chenyan Xiong, Corby Rosset, Arnold ...   \n",
       "4   [Parishad BehnamGhader, Santiago Miret, Siva R...   \n",
       "5   [Zhihong Shao, Yeyun Gong, Yelong Shen, Minlie...   \n",
       "6   [Hao Wang, Guosheng Lin, Steven C. H. Hoi, Chu...   \n",
       "7                  [Tom van Sonsbeek, Marcel Worring]   \n",
       "8   [Jiongnan Liu, Jiajie Jin, Zihan Wang, Jiehan ...   \n",
       "9   [Yifan Gao, Qingyu Yin, Zheng Li, Rui Meng, To...   \n",
       "10  [Mingwen Dong, Christos Christodoulopoulos, Sh...   \n",
       "11                      [Ohad Rubin, Jonathan Berant]   \n",
       "12  [Zhicheng Guo, Sijie Cheng, Yile Wang, Peng Li...   \n",
       "13  [Wenhao Yu, Chenguang Zhu, Zhihan Zhang, Shuoh...   \n",
       "14  [Md Rizwan Parvez, Wasi Uddin Ahmad, Saikat Ch...   \n",
       "15  [Shuo Li, Sangdon Park, Insup Lee, Osbert Bast...   \n",
       "16                            [Xiao Wang, Guo-Jun Qi]   \n",
       "17  [Dong Li, Yelong Shen, Ruoming Jin, Yi Mao, Ku...   \n",
       "18  [Tong Ye, Lingfei Wu, Tengfei Ma, Xuhong Zhang...   \n",
       "19  [Ruochen Zhao, Hailin Chen, Weishi Wang, Fangk...   \n",
       "20  [Yuning Mao, Pengcheng He, Xiaodong Liu, Yelon...   \n",
       "21  [Mercy Ranjit, Gopinath Ganapathy, Ranjit Manu...   \n",
       "22  [Sebastian Hofstätter, Jiecao Chen, Karthik Ra...   \n",
       "23  [Boxin Wang, Wei Ping, Peng Xu, Lawrence McAfe...   \n",
       "24  [Xin Cheng, Di Luo, Xiuying Chen, Lemao Liu, D...   \n",
       "25  [Sunwoo Kim, Kyuhong Shim, Luong Trung Nguyen,...   \n",
       "26  [Dahlia Shehata, Negar Arabzadeh, Charles L. A...   \n",
       "27       [Rita Ramos, Desmond Elliott, Bruno Martins]   \n",
       "28  [Renat Aksitov, Chung-Ching Chang, David Reitt...   \n",
       "29  [Sreyan Ghosh, Utkarsh Tyagi, Sonal Kumar, Din...   \n",
       "30                         [Min Sik Oh, Min Sang Kim]   \n",
       "31         [Panupong Pasupat, Yuan Zhang, Kelvin Guu]   \n",
       "32  [Tao Shen, Guodong Long, Xiubo Geng, Chongyang...   \n",
       "33       [Shengyao Zhuang, Linjun Shou, Guido Zuccon]   \n",
       "34  [Zheng Yuan, Qiao Jin, Chuanqi Tan, Zhengyun Z...   \n",
       "35  [Bill Yuchen Lin, Kangmin Tan, Chris Miller, B...   \n",
       "36  [Haochen Li, Chunyan Miao, Cyril Leung, Yanxia...   \n",
       "37        [Jianwei Niu, Hok Shing Wong, Tieyong Zeng]   \n",
       "38  [Young Kyun Jang, Geonmo Gu, Byungsoo Ko, Isaa...   \n",
       "39  [Alexander Long, Wei Yin, Thalaiyasingam Ajant...   \n",
       "40  [Jiahua Rao, Zifei Shan, Longpo Liu, Yao Zhou,...   \n",
       "41  [Minki Kang, Jin Myung Kwak, Jinheon Baek, Sun...   \n",
       "42  [Sheng-Chieh Lin, Akari Asai, Minghan Li, Barl...   \n",
       "43  [Max F. Burg, Florian Wenzel, Dominik Zietlow,...   \n",
       "44  [Rui Meng, Ye Liu, Semih Yavuz, Divyansh Agarw...   \n",
       "45  [Ensheng Shi, Yanlin Wang, Wei Tao, Lun Du, Ho...   \n",
       "46  [Yury Zemlyanskiy, Michiel de Jong, Joshua Ain...   \n",
       "47                           [Weizhe Lin, Bill Byrne]   \n",
       "48  [Chia-Chien Hung, Tommaso Green, Robert Litsch...   \n",
       "49  [Tanay Dixit, Bhargavi Paranjape, Hannaneh Haj...   \n",
       "\n",
       "                                              Summary  \n",
       "0   Recently, retrieval-augmented text generation ...  \n",
       "1   Despite the remarkable ability of large langua...  \n",
       "2   Retrieval augmentation can aid language models...  \n",
       "3   In this paper we improve the zero-shot general...  \n",
       "4   Augmenting pretrained language models with ret...  \n",
       "5   Large language models are powerful text proces...  \n",
       "6   This paper investigates an open research probl...  \n",
       "7   An important component of human analysis of me...  \n",
       "8   Although Large Language Models (LLMs) have dem...  \n",
       "9   Keyphrase generation is the task of automatica...  \n",
       "10  Accurate evidence retrieval is essential for a...  \n",
       "11  Retrieval-augmented language models (LMs) have...  \n",
       "12  Retrieval-augmented methods have received incr...  \n",
       "13  A common thread of retrieval-augmented methods...  \n",
       "14  Software developers write a lot of source code...  \n",
       "15  Although conversational AIs have demonstrated ...  \n",
       "16  Representation learning has significantly been...  \n",
       "17  Pre-trained language models have achieved prom...  \n",
       "18  Automatically generating human-readable text d...  \n",
       "19  In this survey, we review methods that retriev...  \n",
       "20  We propose Generation-Augmented Retrieval (GAR...  \n",
       "21  We propose Retrieval Augmented Generation (RAG...  \n",
       "22  Retrieval-augmented generation models offer ma...  \n",
       "23  Large decoder-only language models (LMs) can b...  \n",
       "24  With direct access to human-written reference ...  \n",
       "25  Image text retrieval is a task to search for t...  \n",
       "26  Despite the advantages of their low-resource s...  \n",
       "27  Inspired by retrieval-augmented language gener...  \n",
       "28  Despite recent progress, it has been difficult...  \n",
       "29  Biomedical Named Entity Recognition (BioNER) i...  \n",
       "30  Persona and Knowledge dual context open-domain...  \n",
       "31  In practical applications of semantic parsing,...  \n",
       "32  In this work, we propose a simple method that ...  \n",
       "33  Effective cross-lingual dense retrieval method...  \n",
       "34  Vision-and-language multi-modal pretraining an...  \n",
       "35  Humans can perform unseen tasks by recalling r...  \n",
       "36  Code search, which aims at retrieving the most...  \n",
       "37  Phase retrieval is an important problem with s...  \n",
       "38  In hash-based image retrieval systems, degrade...  \n",
       "39  We introduce Retrieval Augmented Classificatio...  \n",
       "40  With recent progress in large-scale vision and...  \n",
       "41  Language models have achieved impressive perfo...  \n",
       "42  Various techniques have been developed in rece...  \n",
       "43  Diffusion models excel at generating photoreal...  \n",
       "44  Dense retrievers have made significant strides...  \n",
       "45  Commit messages are important for software dev...  \n",
       "46  A common recent approach to semantic parsing a...  \n",
       "47  Outside-Knowledge Visual Question Answering (O...  \n",
       "48  This paper introduces our proposed system for ...  \n",
       "49  Counterfactual data augmentation (CDA) -- i.e....  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "fade210a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.document_loaders import ArxivLoader\n",
    "\n",
    "docs = ArxivLoader(query=\"2307.00476\", load_max_docs=2).load()\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fd19596a",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = arxiv.Result(docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e22c4ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "21229385",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ls_files = files_in_dir('./data/arxiv/tar/',['.pdf'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "3081f69a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [14:24<00:00, 17.29s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import arxiv\n",
    "path_paper = './data/arxiv/tar'\n",
    "\n",
    "for d in tqdm(docs):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "17dee291",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://arxiv.org/abs/2202.01110v2'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]['entry_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b847d095",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./2305.14975v1.Just_Ask_for_Calibration_Strategies_for_Eliciting_Calibrated_Confidence_Scores_from_Language_Models_Fine_Tuned_with_Human_Feedback.pdf'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "http://arxiv.org/abs/2202.01110v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad8549a",
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "8668f179",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9], [10, 11, 12, 13, 14]]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ls = [i for i in range(15)]\n",
    "from math import ceil\n",
    "def chunk_into_size(lst, size):\n",
    "\n",
    "    n =ceil(len(lst) / size)\n",
    "    return list(\n",
    "        map(lambda x: lst[x * size:x * size + size],\n",
    "        list(range(n)))\n",
    "        )\n",
    "\n",
    "chunk_into_size(ls, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "3b70e668",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = \"../../2307.00476.pdf\"\n",
    "pdf_path = './data/arxiv/tar/2202.01110v2.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "c32d91ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./data/arxiv/tar/2108.11601v2.pdf',\n",
       " './data/arxiv/tar/2204.07937v2.pdf',\n",
       " './data/arxiv/tar/2205.10471v2.pdf',\n",
       " './data/arxiv/tar/2210.03809v2.pdf',\n",
       " './data/arxiv/tar/2212.09146v2.pdf',\n",
       " './data/arxiv/tar/2305.10647v1.pdf',\n",
       " './data/arxiv/tar/2302.05578v2.pdf',\n",
       " './data/arxiv/tar/2307.04642v1.pdf',\n",
       " './data/arxiv/tar/2209.14290v1.pdf',\n",
       " './data/arxiv/tar/2009.08553v4.pdf',\n",
       " './data/arxiv/tar/2207.13919v1.pdf',\n",
       " './data/arxiv/tar/2303.05692v1.pdf',\n",
       " './data/arxiv/tar/2202.11233v1.pdf',\n",
       " './data/arxiv/tar/2304.14233v1.pdf',\n",
       " './data/arxiv/tar/2302.08268v1.pdf',\n",
       " './data/arxiv/tar/2202.01110v2.pdf',\n",
       " './data/arxiv/tar/2305.11074v1.pdf',\n",
       " './data/arxiv/tar/2305.02437v2.pdf',\n",
       " './data/arxiv/tar/2303.10868v1.pdf',\n",
       " './data/arxiv/tar/2302.07452v1.pdf',\n",
       " './data/arxiv/tar/2305.18846v1.pdf',\n",
       " './data/arxiv/tar/2208.04887v2.pdf',\n",
       " './data/arxiv/tar/2110.08458v2.pdf',\n",
       " './data/arxiv/tar/2305.17331v1.pdf',\n",
       " './data/arxiv/tar/2207.14428v1.pdf',\n",
       " './data/arxiv/tar/2212.08841v2.pdf',\n",
       " './data/arxiv/tar/2210.04873v2.pdf',\n",
       " './data/arxiv/tar/2112.07618v1.pdf',\n",
       " './data/arxiv/tar/2203.02700v3.pdf',\n",
       " './data/arxiv/tar/2210.12285v1.pdf',\n",
       " './data/arxiv/tar/2306.05212v1.pdf',\n",
       " './data/arxiv/tar/2305.03660v1.pdf',\n",
       " './data/arxiv/tar/2205.00834v1.pdf',\n",
       " './data/arxiv/tar/2104.07713v2.pdf',\n",
       " './data/arxiv/tar/2305.06983v1.pdf',\n",
       " './data/arxiv/tar/2305.03950v1.pdf',\n",
       " './data/arxiv/tar/2205.14981v1.pdf',\n",
       " './data/arxiv/tar/2112.08816v2.pdf',\n",
       " './data/arxiv/tar/2304.13923v1.pdf',\n",
       " './data/arxiv/tar/2304.10253v1.pdf',\n",
       " './data/arxiv/tar/2303.00534v1.pdf',\n",
       " './data/arxiv/tar/2306.13421v1.pdf',\n",
       " './data/arxiv/tar/2210.12887v1.pdf',\n",
       " './data/arxiv/tar/2305.15294v1.pdf',\n",
       " './data/arxiv/tar/2305.17653v1.pdf',\n",
       " './data/arxiv/tar/2212.10692v1.pdf',\n",
       " './data/arxiv/tar/2302.11352v1.pdf',\n",
       " './data/arxiv/tar/2304.06762v1.pdf',\n",
       " './data/arxiv/tar/2209.14899v1.pdf',\n",
       " './data/arxiv/tar/2302.03754v1.pdf']"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ls_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84cf4497",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|███████████████████████████████████████████████████████████████████████████████████████▌                                                          | 30/50 [00:03<00:03,  6.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing/Multiple REFRN: found 2 or ABSTR: found 1\n",
      "['abstract', 'references', 'references']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|████████████████████████████████████████████████████████████████████████████████████████████████▎                                                 | 33/50 [00:04<00:03,  4.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing/Multiple REFRN: found 0 or ABSTR: found 0\n",
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 68%|███████████████████████████████████████████████████████████████████████████████████████████████████▎                                              | 34/50 [00:04<00:03,  4.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing/Multiple REFRN: found 1 or ABSTR: found 0\n",
      "['references']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                   | 38/50 [00:08<00:08,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing/Multiple REFRN: found 1 or ABSTR: found 0\n",
      "['references']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏        | 47/50 [00:15<00:01,  1.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing/Multiple REFRN: found 1 or ABSTR: found 0\n",
      "['references']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:16<00:00,  2.95it/s]\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "output_path = './data/arxiv/clean_txt/'\n",
    "from rsch.utils_pdf_clean import prep_docs\n",
    "\n",
    "ls_files = files_in_dir('./data/arxiv/tar/',['.pdf'])\n",
    "\n",
    "for pdf_path in tqdm(ls_files):\n",
    "    pdf_id = pdf_path.split('/')[-1].replace('.pdf','')\n",
    "    path_res = os.path.join(output_path, f'gpt_pp01_{pdf_id}.json')\n",
    "    #path_outline = os.path.join(output_path, f'gpt_pp01_{pdf_id}_outline.json')\n",
    "    if not os.path.exists(path_res):\n",
    "        \n",
    "        loader = PyPDFLoader(pdf_path)\n",
    "        #pages = loader.load_and_split()\n",
    "        docs = loader.load()\n",
    "        paper_json = prep_docs(docs)\n",
    "\n",
    "        json.dump(paper_json,open(path_res,'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "9cfaf276",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1 Introduction',\n",
       " '2 Retrieval-Augmented Paradigm',\n",
       " '2.1 Formulation',\n",
       " '2.3 Retrieval Metrics',\n",
       " '2.4 Integration',\n",
       " '3 Dialogue Response Generation',\n",
       " '4 Machine Translation',\n",
       " '4.1 Translation Memory in SMT',\n",
       " '4.2 Translation Memory in NMT',\n",
       " '5 Other Tasks',\n",
       " '6 Future Directions',\n",
       " '7 Conclusion',\n",
       " 'Abstract']"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[p['title'] for p in paper_json]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e38c95d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "loader = PyMuPDFLoader(pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9873817",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.document_loaders import PDFPlumberLoader\n",
    "loader = PDFPlumberLoader(pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "96e70488",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "6fd2fa5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./data/arxiv/tar/2202.01110v2.pdf'"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "a6cc21ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "ebea93f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8befcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "- does it have enough spaces\n",
    "- if there is a chart that contains text, can we safely remove it\n",
    "- is this page references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "de2d6d8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.Series([len(t) for t in ls_text]).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "edf20b7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Huayang Li~;\\x03Yixuan Su\\x7f;\\x03Deng Cai};\\x03Yan Wang|;\\x03Lemao Liu|;\\x03',\n",
       " '~Nara Institute of Science and Technology\\x7fUniversity of Cambridge',\n",
       " '\\x03All authors contributed equally.ﬁrstly present the generic paradigm of retrieval-',\n",
       " 'augmented generation paradigm, including the re-arXiv:2202.01110v2  [cs.CL]  13 Feb 2022',\n",
       " 'Sources (Sec. 2.2):Training CorpusExternal DataUnsupervised DataMetrics(Sec. 2.3):Sparse-vector RetrievalDense-vector RetrievalTask-specific RetrievalRetrieval MemoryGeneration ModelSec. 4: Machine TranslationSec. 5: Other TasksData AugmentationAttention MechanismSkeleton & TemplatesInformation RetrievalTasks:Sec. 3: Dialogue GenerationModels (Sec 2.4):OutputFigure 1: The overview of this survey.',\n",
       " 'pose a cross-lingual retriever to directly retrieve tar-',\n",
       " '2018; Bulte and Tezcan, 2019).Attention Mechanisms Another integration',\n",
       " 'stances. Due to the one-to-many nature, it hap-pens frequently that a retrieved response (extracted',\n",
       " 'graded largely, because it explicitly isolates TMmatching and SMT decoding and reuses the results',\n",
       " '<t)is thehidden state at each time step when translating yr',\n",
       " 'We refer readers interested in QA to Chen and Yih (2020).approach. During learning, an input sequence to',\n",
       " 'address such an issue on similarity.Retrieval Efﬁciency Generally, if one enlarges',\n",
       " 'researchers to investigate the possibility of retrieval-',\n",
       " 'Papers) , pages 1219–1228.Deng Cai, Yan Wang, Wei Bi, Zhaopeng Tu, Xiao-',\n",
       " '4904–4916. PMLR.Andrej Karpathy and Fei-Fei Li. 2015. Deep visual-',\n",
       " 'statistical machine translation. In Proceedings of the41st Annual Meeting of the Association for Compu-',\n",
       " 'AI, pages 87–92.Yu Wu, Furu Wei, Shaohan Huang, Yunli Wang, Zhou-']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[t for t in ls_text if len(t)>55]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "afad472d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1 Introduction',\n",
       " '2 Retrieval-Augmented Paradigm',\n",
       " '2.1 Formulation',\n",
       " '2.3 Retrieval Metrics',\n",
       " '2.4 Integration',\n",
       " '3 Dialogue Response Generation',\n",
       " '4 Machine Translation',\n",
       " '4.1 Translation Memory in SMT',\n",
       " '4.2 Translation Memory in NMT',\n",
       " '5 Other Tasks',\n",
       " '6 Future Directions',\n",
       " '7 Conclusion',\n",
       " '465. Springer.',\n",
       " '2019 Conference on Empirical Methods in Natu-',\n",
       " '2018. Retrieve, rerank and rewrite: Soft template',\n",
       " '2021 Conference of the North American Chapter of',\n",
       " '2019. Word position aware translation memory for',\n",
       " '2021. Internet-augmented dialogue generation.',\n",
       " '2019. Latent retrieval for weakly supervised',\n",
       " '1 (Long Papers) , pages 1865–1874. Association for',\n",
       " '260.',\n",
       " '2012 Joint Conference on Empirical Methods in Nat-',\n",
       " '20 November, 2021 , pages 910–917. Association for',\n",
       " '2018. Retrieve and reﬁne: Improved sequence gen-',\n",
       " '2018 EMNLP Workshop SCAI: The 2nd Interna-',\n",
       " '1350.',\n",
       " '2020. Neural machine translation with universal',\n",
       " '2021 , pages 4234–4241.']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[t for t in ls_text if is_title(t)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a23335",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rsch_prompts.map_reduce_summary import map_reduce_summary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b3e71f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ARXIV_MAX_QUERY_LENGTH = 300\n",
    "\n",
    "query = 'SNP'\n",
    "top_k_results = 20\n",
    "docs_snp = [\n",
    "    {\"published\": result.published.date(),\n",
    "     \"updated\": result.updated.date(),\n",
    "     \"entry_id\": result.entry_id,\n",
    "     \"Title\": result.title,\n",
    "    f\"Authors\": [a.name for a in result.authors],\n",
    "    f\"Summary\": result.summary}\n",
    "    for result in arxiv.Search(  # type: ignore\n",
    "        query[: ARXIV_MAX_QUERY_LENGTH], max_results=top_k_results\n",
    "    ).results()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6a14039d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single-nucleotide polymorphisms (SNPs) account for most variations between human genomes\n",
      "We show how, if the genomes in a database differ only by a reasonable number of SNPs and the substrings between those SNPs are unique, then we can store a fast compressed suffix array for that database.\n",
      "\n",
      "Single nucleotide polymorphisms (SNPs) often appear in clusters along the length of a chromosome\n",
      "This is due to variation in local coalescent times caused by,for example, selection or recombination\n",
      "Here we investigate whether recombination alone (within a neutral model) can cause statistically significant SNP clustering\n",
      "We measure the extent of SNP clustering as the ratio between the variance of SNPs found in bins of length $l$, and the mean number of SNPs in such bins, $\\sigma^2_l/\\mu_l$\n",
      "For a uniform SNP distribution $\\sigma^2_l/\\mu_l=1$, for clustered SNPs $\\sigma^2_l/\\mu_l > 1$\n",
      "Apart from the bin length, three length scales are important when accounting for SNP clustering: The mean distance between neighboring SNPs, $\\Delta$, the mean length of chromosome segments with constant time to the most recent common ancestor, $\\el$, and the total length of the chromosome, $L$\n",
      "We show that SNP clustering is observed if $\\Delta < \\el \\ll L$\n",
      "Moreover, if $l\\ll \\el \\ll L$, clustering becomes independent of the rate of recombination\n",
      "We apply our results to the analysis of SNP data sets from mice, and human chromosomes 6 and X\n",
      "Of the three data sets investigated, the human X chromosome displays the most significant deviation from neutrality.\n",
      "\n",
      "We present two results about using allele-count (AC) burdens of rare SNPs discovered in a case-control sequencing study for prediction or validation in an external prospective study\n",
      "When genotyping only the SNPs polymorphic in the sequence data, the phenotype to AC correlation tends to be larger in the replication data than the primary study\n",
      "Conversely, if the replication sample is sequenced, ACs of SNPs which are novel in the replication tend to have much smaller or opposite signed associations\n",
      "We explain this by first deriving the AC-phenotype association implied by a model of diverse SNP effects, and second accounting for the shifted distribution of SNP effects when using a case-control study as a filter for SNP inclusion\n",
      "In rare diseases, the case population is depleted of protective SNPs and enriched for deleterious SNPs, creating the above difference in AC associations\n",
      "This phenomenon is most relevant in re-sequencing for risk prediction in rare diseases with heterogeneous rare mutations because it applies to SNPs with MAF near 1 out of the case-control sample size and is exaggerated when SNP log-odds ratios come from a heavy-tailed distribution\n",
      "It also suggests a ``winner's curse'' in which most risk increasing SNPs at a particular MAF are quickly discovered and future sequencing finds more protective or irrelevant SNPs.\n",
      "\n",
      "In this paper, association results from genome-wide association studies (GWAS) are combined with a deep learning framework to test the predictive capacity of statistically significant single nucleotide polymorphism (SNPs) associated with obesity phenotype\n",
      "Our approach demonstrates the potential of deep learning as a powerful framework for GWAS analysis that can capture information about SNPs and the important interactions between them\n",
      "Basic statistical methods and techniques for the analysis of genetic SNP data from population-based genome-wide studies have been considered\n",
      "Statistical association testing between individual SNPs and obesity was conducted under an additive model using logistic regression\n",
      "Four subsets of loci after quality-control (QC) and association analysis were selected: P-values lower than 1x10-5 (5 SNPs), 1x10-4 (32 SNPs), 1x10-3 (248 SNPs) and 1x10-2 (2465 SNPs)\n",
      "A deep learning classifier is initialised using these sets of SNPs and fine-tuned to classify obese and non-obese observations\n",
      "Using a deep learning classifier model and genetic variants with P-value < 1x10-2 (2465 SNPs) it was possible to obtain results (SE=0.9604, SP=0.9712, Gini=0.9817, LogLoss=0.1150, AUC=0.9908 and MSE=0.0300)\n",
      "As the P-value increased, an evident deterioration in performance was observed\n",
      "Results demonstrate that single SNP analysis fails to capture the cumulative effect of less significant variants and their overall contribution to the outcome in disease prediction, which is captured using a deep learning framework.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for d in docs_snp[:4]:\n",
    "    [print(do) for do in d['Summary'].replace('\\n',' ').split('. ')]\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346d2512",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "similacra",
   "language": "python",
   "name": "similacra"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
